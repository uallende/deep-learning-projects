{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 09:51:24.734534: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-11 09:51:25.195633: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/root/miniconda3/envs/tflow/lib/:/root/miniconda3/envs/tflow/lib/python3.10/site-packages/nvidia/cudnn/lib\n",
      "2023-07-11 09:51:25.197828: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/root/miniconda3/envs/tflow/lib/:/root/miniconda3/envs/tflow/lib/python3.10/site-packages/nvidia/cudnn/lib\n",
      "2023-07-11 09:51:25.197838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/root/miniconda3/envs/tflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /root/miniconda3/envs/tflow/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /root/miniconda3/envs/tflow/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "--2023-07-11 09:51:26--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2016/mono/es.txt.gz\n",
      "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
      "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1859673728 (1.7G) [application/gzip]\n",
      "Saving to: ‘./data/OpenSubtitles.txt.gz’\n",
      "\n",
      "./data/OpenSubtitle 100%[===================>]   1.73G  32.5MB/s    in 66s     \n",
      "\n",
      "2023-07-11 09:52:33 (26.8 MB/s) - ‘./data/OpenSubtitles.txt.gz’ saved [1859673728/1859673728]\n",
      "\n",
      "/bin/bash: /root/miniconda3/envs/tflow/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /root/miniconda3/envs/tflow/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /root/miniconda3/envs/tflow/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"./data\" \n",
    "TRAIN_SIZE = 500_000\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "  !mkdir -p $DATA_DIR\n",
    "  !wget \"https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2016/mono/es.txt.gz\" -O $DATA_DIR/OpenSubtitles.txt.gz\n",
    "  !gzip -d $DATA_DIR/OpenSubtitles.txt.gz\n",
    "  !head -n $TRAIN_SIZE $DATA_DIR/OpenSubtitles.txt > $DATA_DIR/train_data.txt \n",
    "  !rm $DATA_DIR/OpenSubtitles.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4584392"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #load a chunk of the dataset from hugging face\n",
    "# dataset = load_dataset('./spanish_billion_words.py')['train'].select(range(75_000))\n",
    "# # save the dataset to a text file\n",
    "# with open(\"dataset.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "#     for example in dataset:\n",
    "#         file.write(example[\"text\"] + \"\\n\")\n",
    "\n",
    "vocab_size = 6000\n",
    "tokenizer = Tokenizer(BPE())\n",
    "# Configure the trainer\n",
    "trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"\\n\"], vocab_size=vocab_size)\n",
    "files = ['/mnt/c/Users/UrkoAllende/OneDrive - AMC/learning-projects/Tensorflow/Transformers/dataset.txt']\n",
    "files = ['/mnt/c/Users/UrkoAllende/OneDrive - AMC/learning-projects/Tensorflow/Transformers/data/train_data.txt']\n",
    "\n",
    "\n",
    "with open(files[0], 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "tokenizer.train(files, trainer)\n",
    "tokenizer.save(\"my_custom_bpe_tokenizer.json\")\n",
    "\n",
    "tokens = tokenizer.encode(text).ids\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[304, 3562, 70, 34, 5, 304, 1595, 577, 34, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: ((4125910, 48), (4125910, 48))\n",
      "Validation size: ((458434, 48), (458434, 48))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 09:54:05.176156: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 792174720 exceeds 10% of free system memory.\n",
      "2023-07-11 09:54:05.313200: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 792174720 exceeds 10% of free system memory.\n",
      "2023-07-11 09:54:05.411578: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 792174720 exceeds 10% of free system memory.\n",
      "2023-07-11 09:54:05.555955: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 792174720 exceeds 10% of free system memory.\n",
      "2023-07-11 09:54:05.632814: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 792174720 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: tf.Tensor(\n",
      "[[ 249 4775 1978 ...   34    5   38]\n",
      " [3997  130   19 ... 1079 4776    5]\n",
      " [ 130   34    5 ...   34    5  176]\n",
      " ...\n",
      " [  19    5   54 ...    5 4585  154]\n",
      " [  19    5   48 ...   34    5  875]\n",
      " [3260  605 1401 ...   19    5 2211]], shape=(512, 48), dtype=int32)\n",
      "Labels: tf.Tensor(\n",
      "[[4775 1978  252 ...    5   38 2849]\n",
      " [ 130   19    5 ... 4776    5  823]\n",
      " [  34    5 4236 ...    5  176  247]\n",
      " ...\n",
      " [   5   54 3331 ... 4585  154  754]\n",
      " [   5   48  130 ...    5  875 1917]\n",
      " [ 605 1401  170 ...    5 2211 2606]], shape=(512, 48), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Set sequence length and split ratio for validation set\n",
    "sequence_length = 48\n",
    "val_split = 0.1\n",
    "batch_size = 512\n",
    "\n",
    "# Create xs and ys\n",
    "xs = []\n",
    "ys = []\n",
    "for i in range(len(tokens) - sequence_length):\n",
    "    xs.append(tokens[i:i + sequence_length])\n",
    "    ys.append(tokens[i+1 : i+1+ sequence_length])\n",
    "\n",
    "# Convert xs and ys to numpy arrays\n",
    "xs = np.array(xs)\n",
    "ys = np.array(ys)\n",
    "\n",
    "val_size = int(len(xs) * val_split)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train, x_val = xs[:-val_size].astype(np.int32), xs[-val_size:].astype(np.int32)\n",
    "y_train, y_val = ys[:-val_size].astype(np.int32), ys[-val_size:].astype(np.int32)\n",
    "\n",
    "del xs, ys\n",
    "\n",
    "print(f'Training size: {x_train.shape, y_train.shape}')\n",
    "print(f'Validation size: {x_val.shape, y_val.shape}')\n",
    "\n",
    "def cast_to_int32(features, labels):\n",
    "    features = (tf.cast(features[0], tf.int32), tf.cast(features[1], tf.int32))\n",
    "    labels = tf.cast(labels, tf.int32)\n",
    "    return features, labels\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((x_train, x_train), y_train)).shuffle(buffer_size=10000).batch(batch_size)\n",
    "train_dataset = train_dataset.map(cast_to_int32)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(((x_val, x_val), y_val)).batch(batch_size)\n",
    "val_dataset = val_dataset.map(cast_to_int32)\n",
    "\n",
    "del x_train, x_val, y_train, y_val\n",
    "\n",
    "# Take a single sample from the dataset\n",
    "sample = train_dataset.take(1)\n",
    "\n",
    "for data in sample:\n",
    "    features = data[0][0]\n",
    "    labels = data[1]\n",
    "    print(\"Features:\", features)\n",
    "    print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "  def compute_mask(self, *args, **kwargs):\n",
    "    return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "  def call(self, x):\n",
    "    length = tf.shape(x)[1]\n",
    "    x = self.embedding(x)\n",
    "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "    return x\n",
    "  \n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "  \n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dff, dropout_rate=0.25):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x\n",
    "  \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = self.causal_self_attention(x=x)\n",
    "\n",
    "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    return x\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.25):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                             d_model=d_model)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dec_layers = [\n",
    "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "    self.last_attn_scores = None\n",
    "\n",
    "  def call(self, x, context):\n",
    "    # `x` is token-IDs shape (batch, target_seq_len)\n",
    "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x  = self.dec_layers[i](x, context)\n",
    "\n",
    "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "    return x\n",
    "  \n",
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.25):\n",
    "    super().__init__()\n",
    "\n",
    "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "    # first argument.\n",
    "    context, x  = inputs\n",
    "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "    # Final linear layer output.\n",
    "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "    try:\n",
    "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "      # b/250038731\n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    # Return the final output and the attention weights.\n",
    "    return logits\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "  \n",
    "\n",
    "def masked_loss(label, pred):\n",
    "  mask = label != 0\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "  loss = loss_object(label, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss *= mask\n",
    "\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "  match = label == pred\n",
    "\n",
    "  mask = label != 0\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder (Decoder)           multiple                  2352128   \n",
      "                                                                 \n",
      " dense_8 (Dense)             multiple                  774000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,126,128\n",
      "Trainable params: 3,126,128\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 128 * 4\n",
    "num_heads = 4\n",
    "dropout_rate = 0.4\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=vocab_size,\n",
    "    target_vocab_size=vocab_size,\n",
    "    dropout_rate=dropout_rate)\n",
    "\n",
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])\n",
    "\n",
    "output = transformer((features, labels))\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8059/8059 [==============================] - 2420s 300ms/step - loss: 3.6351 - masked_accuracy: 0.3957 - val_loss: 5.8066 - val_masked_accuracy: 0.2556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0c5587cf70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(train_dataset,\n",
    "                epochs=1,\n",
    "                validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asegúrate de pronunciar estas frases con el acento español correcto.\n",
      "Quisiera ser discreda.\n",
      "Eso es bueno.\n",
      "No puede salir el autobus que dices.\n",
      "Un buen chiste.\n",
      "Creo que yo diría que no\n",
      "Buena eleccionaré y decides salir a Gunther.\n",
      "\"Este chico es algo herido y una cita, así que dijiste que eras tan tarde a la madre.\n",
      "Excepto las manos en el mundo.\n",
      "No tiré a punto de reír.\n",
      "Y tu tambien.\n",
      "- Esta bien.\n",
      "Por favor, Gracias.\n",
      "Espara, pero no puedo tragar cosas.\n",
      "- ¿Quien?\n",
      "Quedense hallar cuentas, asi que propondi una chada para allá.\n",
      "- Creo que fue mandona.\n",
      "- Si, creo que necesitas...\n",
      "- Yo ya no opuesto.\n",
      "- Ya ve.\n",
      "Están aquí.\n",
      "- De acuerdo.\n",
      "Hay buen dinero.\n",
      "- Buenas noches, Babbette.\n",
      "- Hola, chicas.\n",
      "- Hola, chicos.\n",
      "- Hola, chicas.\n",
      "- No, gracias.\n",
      "- ¿Quién te dijo algo?\n",
      "- Max.\n",
      "Lo se.\n",
      "Por favoria Sookie, lo ha hecho.\n",
      "- Jamas, el Sr. Medina.\n",
      "- ¿Que?\n",
      "¿Mi hermano es dolor de Majestada?\n",
      "Es gené.\n",
      "Si, lo sé. ¿alguien elegiste?\n",
      "Leía la temporada por una hora de la luego.\n",
      "También eres mandona...\n",
      "¿Y quién es manejable?\n",
      "Escubros pronto.\n",
      "Me gusta el examen de trabajo.\n",
      "¿Está bien?\n",
      "- Si.\n",
      "Por favor.\n",
      "¿Podrias tener una casa?\n",
      "¿No cenizas aquia un así?\n",
      "Una vez.\n",
      "Miax tiene razón.\n",
      "Claro.\n",
      "Morey, está de acuerdo con el resto de mi familia.\n",
      "No necesito culpa. ¿De verdad?\n",
      "Genial.\n",
      "Han matado un librado.\n",
      "Es sabio de un par de por papeles.\n",
      "Saben que quisieran ser montor, queremos ser mandona... Antes de ser suyas, pero tiene buen dinero.\n",
      "Y esto tiene gracias.\n",
      "Es el trabajo incluido.\n",
      "- A otro, no importa.\n",
      "No os es...\n",
      "de que un hombre desde que me equivoque.\n",
      "Creí que se sienta.\n",
      "Puedo repetirte ese por uno.\n",
      "- ¿Acaso queria vivir en este trabajo?\n",
      "- No recuerdo nada de eso.\n",
      "Que raro, ¿verdad?\n",
      "- Le dir\n"
     ]
    }
   ],
   "source": [
    "def generate_text(transformer, tokenizer, prompt, max_length=300):\n",
    "    input_tokens = tokenizer.encode(prompt).ids\n",
    "\n",
    "    # while len(input_tokens) < sequence_length:\n",
    "    #     input_tokens.append(0)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor([input_tokens])\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        logits = transformer([input_tensor[:, -sequence_length:], input_tensor[:, -sequence_length:]])[0, -1]\n",
    "        \n",
    "        probs = tf.nn.softmax(logits)\n",
    "        sampled_token = np.random.choice(len(probs), p=probs.numpy())\n",
    "        \n",
    "        input_tokens.append(sampled_token)\n",
    "        input_tensor = tf.convert_to_tensor([input_tokens])\n",
    "\n",
    "    pieces = [tokenizer.id_to_token(token_id) for token_id in input_tokens]\n",
    "    text = ''.join([piece.replace('▁', ' ') for piece in pieces])\n",
    "    \n",
    "    return text\n",
    "\n",
    "prompt = \"Asegúrate de pronunciar estas frases con el acento español correcto.\"  \n",
    "generated_text = generate_text(transformer, tokenizer, prompt, max_length=500)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
