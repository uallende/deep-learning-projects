{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_samples(names, block_size, stoi):\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for name in names:\n",
    "        name = f\"{'.' * block_size}{name}\"\n",
    "\n",
    "        for i in range(len(name) - block_size):\n",
    "            x = [stoi[char] for char in name[i:i + block_size]]\n",
    "            y = stoi[name[i + block_size]]\n",
    "\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "\n",
    "    return torch.tensor(xs, dtype=torch.long), torch.tensor(ys, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open('names.txt').read().split('\\n')\n",
    "\n",
    "vocab = sorted(list(set(list('.'.join(names)))))\n",
    "vocab_size = len(vocab)\n",
    "stoi = {ix:char for char, ix in enumerate(vocab)}\n",
    "itos = {char:ix for char, ix in enumerate(vocab)}\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8*len(names))\n",
    "n2 = int(0.9*len(names))\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 256\n",
    "d_model = 24\n",
    "n_hidden = 128\n",
    "n_flat_steps = 2\n",
    "\n",
    "Xtr,  Ytr  = make_training_samples(names[:n1], block_size, stoi)     # 80%\n",
    "Xdev, Ydev = make_training_samples(names[n1:n2], block_size, stoi)   # 10%\n",
    "Xte,  Yte  = make_training_samples(names[n2:], block_size, stoi)     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data, is_label = None):\n",
    "\n",
    "    x = [stoi[char] for name in data for char in name]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenConsecutive:\n",
    "  \n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape\n",
    "    x = x.reshape(B, T//self.n, C*self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpBatchNorm(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size):\n",
    "        super(MlpBatchNorm, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.linear = nn.Linear(d_model * 2, n_hidden, bias=False)\n",
    "        self.linear2 = nn.Linear(n_hidden * 2, n_hidden, bias=False)\n",
    "        self.linear3 = nn.Linear(n_hidden * 2, n_hidden, bias=False)\n",
    "\n",
    "        self.bnorm = nn.BatchNorm1d(n_hidden)\n",
    "        self.bnorm2 = nn.BatchNorm1d(n_hidden)\n",
    "        self.bnorm3 = nn.BatchNorm1d(n_hidden)\n",
    "\n",
    "        self.flat = FlattenConsecutive(2)\n",
    "        self.out_layer = nn.Linear(n_hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        x = self.flat(x)\n",
    "        x = self.linear(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.bnorm(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.tanh(x)\n",
    "        \n",
    "        x = self.flat(x)\n",
    "        x = self.linear2(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.bnorm2(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.tanh(x)\n",
    "\n",
    "        x = self.flat(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.bnorm3(x)\n",
    "        x = F.tanh(x)\n",
    "        \n",
    "        return self.out_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MlpBatchNorm(\n",
      "  (embedding): Embedding(27, 24)\n",
      "  (linear): Linear(in_features=48, out_features=128, bias=False)\n",
      "  (linear2): Linear(in_features=256, out_features=128, bias=False)\n",
      "  (linear3): Linear(in_features=256, out_features=128, bias=False)\n",
      "  (bnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bnorm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out_layer): Linear(in_features=128, out_features=27, bias=True)\n",
      ")\n",
      "76579\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = MlpBatchNorm(vocab_size, d_model, n_hidden, vocab_size).to(device)\n",
    "print(net)\n",
    "\n",
    "# Enable cuDNN backend for performance optimization\n",
    "if device.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "\n",
    "Xval = Xdev.to(device)\n",
    "Yval = Ydev.to(device)\n",
    "print(sum(p.nelement() for p in net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.3304 --- Val Loss: 3.2947\n",
      "Loss: 2.3732 --- Val Loss: 2.3175\n",
      "Loss: 2.2516 --- Val Loss: 2.2792\n",
      "Loss: 2.2597 --- Val Loss: 2.2488\n",
      "Loss: 2.3274 --- Val Loss: 2.2393\n",
      "Loss: 1.9547 --- Val Loss: 2.2126\n",
      "Loss: 2.1109 --- Val Loss: 2.2051\n",
      "Loss: 2.1979 --- Val Loss: 2.1938\n",
      "Loss: 2.0756 --- Val Loss: 2.1836\n",
      "Loss: 2.1412 --- Val Loss: 2.1884\n",
      "Loss: 2.1585 --- Val Loss: 2.1707\n",
      "Loss: 2.0407 --- Val Loss: 2.1766\n",
      "Loss: 2.2933 --- Val Loss: 2.1684\n",
      "Loss: 2.0360 --- Val Loss: 2.1642\n",
      "Loss: 2.0160 --- Val Loss: 2.1569\n",
      "Loss: 1.9014 --- Val Loss: 2.1547\n",
      "Loss: 1.9958 --- Val Loss: 2.1489\n",
      "Loss: 2.3931 --- Val Loss: 2.1592\n",
      "Loss: 2.1906 --- Val Loss: 2.1517\n",
      "Loss: 2.0623 --- Val Loss: 2.1473\n",
      "Loss: 2.0454 --- Val Loss: 2.1547\n",
      "Loss: 1.8986 --- Val Loss: 2.1013\n",
      "Loss: 1.8259 --- Val Loss: 2.0975\n",
      "Loss: 1.9925 --- Val Loss: 2.0970\n",
      "Loss: 2.0301 --- Val Loss: 2.0922\n",
      "Loss: 2.1724 --- Val Loss: 2.0907\n",
      "Loss: 2.0855 --- Val Loss: 2.0918\n",
      "Loss: 1.8842 --- Val Loss: 2.0917\n",
      "Loss: 1.7608 --- Val Loss: 2.0927\n",
      "Loss: 2.0884 --- Val Loss: 2.0904\n",
      "Loss: 1.9521 --- Val Loss: 2.0885\n",
      "Loss: 1.8118 --- Val Loss: 2.0902\n",
      "Loss: 1.9008 --- Val Loss: 2.0894\n",
      "Loss: 2.0319 --- Val Loss: 2.0890\n",
      "Loss: 1.9184 --- Val Loss: 2.0879\n",
      "Loss: 1.9149 --- Val Loss: 2.0897\n",
      "Loss: 1.9500 --- Val Loss: 2.0902\n",
      "Loss: 1.8745 --- Val Loss: 2.0903\n",
      "Loss: 1.9503 --- Val Loss: 2.0873\n",
      "Loss: 1.9777 --- Val Loss: 2.0858\n",
      "Loss: 2.0567 --- Val Loss: 2.0874\n",
      "Loss: 2.1866 --- Val Loss: 2.0875\n",
      "Loss: 1.9789 --- Val Loss: 2.0894\n",
      "Loss: 1.9404 --- Val Loss: 2.0894\n",
      "Loss: 1.8883 --- Val Loss: 2.0888\n",
      "Loss: 2.1024 --- Val Loss: 2.0885\n",
      "Loss: 1.7925 --- Val Loss: 2.0862\n",
      "Loss: 2.2041 --- Val Loss: 2.0912\n",
      "Loss: 2.0109 --- Val Loss: 2.0876\n",
      "Loss: 1.8212 --- Val Loss: 2.0925\n",
      "Loss: 1.9104 --- Val Loss: 2.0894\n",
      "Loss: 1.8805 --- Val Loss: 2.0904\n",
      "Loss: 1.6366 --- Val Loss: 2.0878\n",
      "Loss: 1.8493 --- Val Loss: 2.0895\n",
      "Loss: 1.7128 --- Val Loss: 2.0892\n",
      "Loss: 2.1640 --- Val Loss: 2.0879\n",
      "Loss: 1.8229 --- Val Loss: 2.0892\n",
      "Loss: 1.7899 --- Val Loss: 2.0860\n",
      "Loss: 1.8425 --- Val Loss: 2.0870\n",
      "Loss: 1.9697 --- Val Loss: 2.0900\n",
      "Loss: 2.2426 --- Val Loss: 2.0876\n",
      "Loss: 2.0494 --- Val Loss: 2.0873\n",
      "Loss: 1.8567 --- Val Loss: 2.0902\n",
      "Loss: 2.0668 --- Val Loss: 2.0885\n",
      "Loss: 2.0106 --- Val Loss: 2.0867\n",
      "Loss: 1.9761 --- Val Loss: 2.0891\n",
      "Loss: 1.9889 --- Val Loss: 2.0858\n",
      "Loss: 1.8163 --- Val Loss: 2.0876\n",
      "Loss: 2.0426 --- Val Loss: 2.0876\n",
      "Loss: 1.8539 --- Val Loss: 2.0887\n",
      "Loss: 2.0893 --- Val Loss: 2.0857\n",
      "Loss: 1.9292 --- Val Loss: 2.0853\n",
      "Loss: 1.7404 --- Val Loss: 2.0863\n",
      "Loss: 1.9565 --- Val Loss: 2.0890\n",
      "Loss: 1.8581 --- Val Loss: 2.0896\n",
      "Loss: 2.0506 --- Val Loss: 2.0881\n",
      "Loss: 1.7077 --- Val Loss: 2.0889\n",
      "Loss: 1.8468 --- Val Loss: 2.0871\n",
      "Loss: 1.7970 --- Val Loss: 2.0887\n",
      "Loss: 1.8842 --- Val Loss: 2.0911\n",
      "Loss: 2.0516 --- Val Loss: 2.0901\n",
      "Loss: 2.2402 --- Val Loss: 2.0902\n",
      "Loss: 1.5235 --- Val Loss: 2.0892\n",
      "Loss: 1.8527 --- Val Loss: 2.0881\n",
      "Loss: 1.7921 --- Val Loss: 2.0888\n",
      "Loss: 2.0251 --- Val Loss: 2.0889\n",
      "Loss: 1.5505 --- Val Loss: 2.0899\n",
      "Loss: 1.9807 --- Val Loss: 2.0888\n",
      "Loss: 2.0202 --- Val Loss: 2.0866\n",
      "Loss: 1.9435 --- Val Loss: 2.0897\n",
      "Loss: 1.8037 --- Val Loss: 2.0913\n",
      "Loss: 2.0829 --- Val Loss: 2.0894\n",
      "Loss: 2.0305 --- Val Loss: 2.0890\n",
      "Loss: 2.1921 --- Val Loss: 2.0929\n",
      "Loss: 1.9719 --- Val Loss: 2.0910\n",
      "Loss: 1.7767 --- Val Loss: 2.0928\n",
      "Loss: 2.0785 --- Val Loss: 2.0933\n",
      "Loss: 1.8742 --- Val Loss: 2.0908\n",
      "Loss: 2.2077 --- Val Loss: 2.0920\n",
      "Loss: 2.0136 --- Val Loss: 2.0936\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "batch_size = 64\n",
    "\n",
    "for i in range(100000):\n",
    "\n",
    "    lr = [0.01 if i >= 20000 else 0.1]\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr[0])\n",
    "\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtr[ix].to(device), Ytr[ix].to(device) \n",
    "\n",
    "    optimizer.zero_grad()   \n",
    "    logits = net(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()    \n",
    "\n",
    "    val_logits = net(Xval)\n",
    "    val_loss = F.cross_entropy(val_logits, Yval)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(f'Loss: {loss:.4f} --- Val Loss: {val_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
