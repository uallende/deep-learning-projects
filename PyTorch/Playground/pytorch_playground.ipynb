{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5]) torch.Size([5, 5]) torch.Size([5, 1]) torch.Size([1, 5]) torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "\n",
    "# Inputs\n",
    "inputs = t.randn(4, 3) # 3 features, 4 samples\n",
    "outputs = t.tensor([-1, 1, 1, -1], dtype=t.float32).view(-1, 1)\n",
    "\n",
    "# Parameters\n",
    "n = 5\n",
    "feat = inputs.shape[1] # 3\n",
    "\n",
    "W1 = t.randn(feat, n, requires_grad=True) # (3, 5) 3 features, 5 neurons\n",
    "B1 = t.randn(1, n, requires_grad=True)  # 1 bias per neuron\n",
    "\n",
    "W2 = t.randn(n, 5, requires_grad=True) #(5,5)\n",
    "B2 = t.randn(1, n, requires_grad=True) #\n",
    "\n",
    "W3 = t.randn(n, 1, requires_grad=True) # (5, 1) 5 neurons, 1 output\n",
    "\n",
    "params = [W1, W2, W3, B1, B2]\n",
    "print(W1.shape, W2.shape, W3.shape, B1.shape, B2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4971133140992.0\n",
      "tensor([[ 3837761.7500],\n",
      "        [  435424.8125],\n",
      "        [ 1365491.5000],\n",
      "        [-1761236.6250]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "L1 = inputs @ W1 + B1 # (4,3) X (3,5) = (4,5)\n",
    "L2 = L1 @ W2 + B2 # (4,5) x (5,5) = (5,5)\n",
    "L3 = L2 @ W3 #(5,5) x (5,1) = (5,1)\n",
    "#L4 = t.randn()\n",
    "\n",
    "# Compute the loss\n",
    "loss = ((L3 - outputs) ** 2).mean()\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "for p in params:\n",
    "    p.grad = None\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "print(L3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "\n",
    "for p in params:\n",
    "    p.data += -lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss: 0.7733758687973022\n",
      "Step 2, Loss: 0.4250103235244751\n",
      "Step 3, Loss: 0.32029587030410767\n",
      "Step 4, Loss: 0.27598363161087036\n",
      "Step 5, Loss: 0.24892061948776245\n",
      "Step 6, Loss: 0.2269206941127777\n",
      "Step 7, Loss: 0.20705828070640564\n",
      "Step 8, Loss: 0.1885438710451126\n",
      "Step 9, Loss: 0.1711898297071457\n",
      "Step 10, Loss: 0.15495039522647858\n",
      "Step 11, Loss: 0.13981766998767853\n",
      "Step 12, Loss: 0.12578465044498444\n",
      "Step 13, Loss: 0.11283937096595764\n",
      "Step 14, Loss: 0.10096020251512527\n",
      "Step 15, Loss: 0.09011775255203247\n",
      "tensor([[-0.6389],\n",
      "        [ 0.8076],\n",
      "        [ 0.5691],\n",
      "        [-0.9141]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Inputs\n",
    "inputs = t.randn(4, 3)  # 3 features, 4 samples\n",
    "outputs = t.tensor([-1, 1, 1, -1], dtype=t.float32).view(-1, 1)\n",
    "\n",
    "# Parameters\n",
    "n = 5\n",
    "feat = inputs.shape[1]  # 3\n",
    "\n",
    "# Initialize weights more carefully\n",
    "W1 = (t.randn(feat, n) / np.sqrt(feat)).clone().detach().requires_grad_(True)\n",
    "B1 = t.randn(1, n).clone().detach().requires_grad_(True)\n",
    "W2 = (t.randn(n, 5) / np.sqrt(n)).clone().detach().requires_grad_(True)\n",
    "B2 = t.randn(1, n).clone().detach().requires_grad_(True)\n",
    "W3 = (t.randn(n, 1) / np.sqrt(n)).clone().detach().requires_grad_(True)\n",
    "\n",
    "params = [W1, W2, W3, B1, B2]\n",
    "\n",
    "lr = 0.03  # Reduce the learning rate\n",
    "\n",
    "# Training loop\n",
    "for i in range(15):  # 100 training steps\n",
    "    # Forward pass\n",
    "    L1 = inputs @ W1 + B1  # (4,3) X (3,5) = (4,5)\n",
    "    L2 = L1 @ W2 + B2  # (4,5) x (5,5) = (4,5)\n",
    "    L3 = L2 @ W3  # (4,5) x (5,1) = (4,1)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = ((L3 - outputs) ** 2).mean()\n",
    "    print(f\"Step {i + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Zero gradients\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            p.grad.zero_()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    with t.no_grad():\n",
    "        for p in params:\n",
    "            p -= lr * p.grad\n",
    "\n",
    "# Visualize the computational graph\n",
    "make_dot(loss, params={name: param for name, param in enumerate(params)}).render(\"computational_graph\", format=\"png\")\n",
    "print(L3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
