{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "d_model = 384\n",
    "block_size = 256\n",
    "batch_size = 32\n",
    "n_heads = 6\n",
    "dropout = 0.1\n",
    "learning_rate = 1e-3\n",
    "epochs = 10_000\n",
    "eval_iters = 200\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, d_model,  dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.att_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1).bool())\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q = x\n",
    "        k = x\n",
    "        v = x\n",
    "        B,T,_ = x.shape \n",
    "        dk = d_model // n_heads\n",
    "\n",
    "        # linear projections\n",
    "        q = self.query(q) \n",
    "        k = self.key(k) \n",
    "        v = self.value(v) \n",
    "\n",
    "        # add number of heads\n",
    "        q = q.view(B,T,n_heads,dk).permute(0,2,1,3)   # B,T,h,dk\n",
    "        k = k.view(B,T,n_heads,dk).permute(0,2,1,3)  \n",
    "        v = v.view(B,T,n_heads,dk).permute(0,2,1,3)  \n",
    "        \n",
    "        # attention \n",
    "        x = q @ k.transpose(-2,-1) # B,h,T,dk @ B,h,dk,T --> B,h,T,T\n",
    "        x = x * dk ** -0.5 # B,h,T,T\n",
    "        x = x.masked_fill(self.mask, float('-inf')) # B,h,T,T\n",
    "        x = F.softmax(x, dim=(-1)) # B,n_h,T,T \n",
    "        x = x @ v  # B,h,T,T @ B,T,h,dv --> B,h,T,dv\n",
    "        B,h,T,dv = x.shape\n",
    "        x = x.transpose(2,1).contiguous().view(B,T,h*dv) #B,T,C\n",
    "        out = self.att_proj(x) # B,T,C\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(block_size, d_model)\n",
    "        self.mha = MultiHeadAttention(n_heads=n_heads, d_model=d_model)\n",
    "        self.lnorm = nn.LayerNorm(d_model)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "\n",
    "        embeds = self.embedding_table(x)\n",
    "        positions = self.pos_embedding(torch.arange(block_size, device=device))\n",
    "        x = embeds + positions\n",
    "\n",
    "        x = self.mha(x)\n",
    "        x = x + self.lnorm(x)\n",
    "        logits = self.out(x)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(input=logits, target=targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        B, T = idx.shape\n",
    "        if T < self.block_size:\n",
    "            # pad the input with zeros if it's less than block_size\n",
    "            idx = F.pad(idx, (0, self.block_size - T))\n",
    "        for _ in range(max_new_tokens):\n",
    "            # use only the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3'].\n",
      "Vocab size: 65\n",
      "very dog to the commonalty.\n",
      "\n",
      "Second Citizen:\n",
      "Consi\n",
      "Training samples: 1003854\n",
      "Validation samples: 111539\n"
     ]
    }
   ],
   "source": [
    "data = open('text.txt').read()\n",
    "vocab = list(sorted(set(data)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'Vocab: {vocab[:10]}.')\n",
    "print(f'Vocab size: {vocab_size}')\n",
    "\n",
    "stoi = {c:i for i, c in enumerate(vocab)}\n",
    "itos = {i:c for i, c in enumerate(vocab)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda i: ''.join([itos[i] for i in i])\n",
    "\n",
    "print(decode(encode(data[1100:1150])))\n",
    "data = torch.tensor(encode(data))\n",
    "\n",
    "n_tr = int(len(data) * 0.9)\n",
    "n_val = len(data) - n_tr\n",
    "\n",
    "train = data[:n_tr]\n",
    "val = data[n_tr+1:]\n",
    "\n",
    "print(f'Training samples: {train.shape[0]}')\n",
    "print(f'Validation samples: {val.shape[0]}')\n",
    "\n",
    "def make_batches(split):\n",
    "\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "Xb, Yb = make_batches('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embedding_table): Embedding(65, 384)\n",
      "  (pos_embedding): Embedding(256, 384)\n",
      "  (mha): MultiHeadAttention(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (query): Linear(in_features=384, out_features=384, bias=False)\n",
      "    (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "    (value): Linear(in_features=384, out_features=384, bias=False)\n",
      "    (att_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  )\n",
      "  (lnorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (out): Linear(in_features=384, out_features=65, bias=True)\n",
      ")\n",
      "Total parameters: 738881\n"
     ]
    }
   ],
   "source": [
    "m = Model(vocab_size, block_size).to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "n_params = sum(p.nelement() for p in m.parameters())\n",
    "print(m)\n",
    "print(f'Total parameters: {n_params}')\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(m):\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = make_batches(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 999. Training Loss: 1.930. Evaluation Loss: 2.063\n",
      "Iteration 1999. Training Loss: 1.840. Evaluation Loss: 2.007\n",
      "Iteration 2999. Training Loss: 1.808. Evaluation Loss: 1.996\n",
      "Iteration 3999. Training Loss: 1.787. Evaluation Loss: 1.968\n",
      "Iteration 4999. Training Loss: 1.772. Evaluation Loss: 1.955\n",
      "Iteration 5999. Training Loss: 1.758. Evaluation Loss: 1.944\n",
      "Iteration 6999. Training Loss: 1.732. Evaluation Loss: 1.930\n",
      "Iteration 7999. Training Loss: 1.718. Evaluation Loss: 1.915\n",
      "Iteration 8999. Training Loss: 1.698. Evaluation Loss: 1.895\n",
      "Iteration 9999. Training Loss: 1.685. Evaluation Loss: 1.891\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    Xb, Yb = make_batches('train')\n",
    "    logits, loss = m(Xb, Yb) # B, C\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 999:\n",
    "        l = estimate_loss(m)\n",
    "        print(f\"Iteration {epoch}. Training Loss: {l['train']:.3f}. Evaluation Loss: {l['val']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLOUGHBY:\n",
      "I trand, weNceise seemvyeS\n",
      "eR\n",
      "ISABELeY:\n",
      "I mineB: it being anst inkengersens: thousiefne!\n",
      "\n",
      "The\n",
      "ther, I'll sucome, him to hences,\n",
      "What eversts this to queet of hem body.\n",
      "\n",
      "CLIDI:\n",
      "Wilesd canfor to pwork murdy; but the sean ence.\n",
      "\n",
      "KING RICHARD III:\n",
      "Murdess trall on, iffee preplewind,\n",
      "Yes, whome, in That from banime bulses such not's the trince tllings frys say's star be to osert!\n",
      "I he dam, o'er tarsklikes tosh have your sight not's is will not whit's not you Juloven tates, but had well had?\n",
      "\n",
      "ROMEO:\n",
      "And inged all frit imptroctio, sod blane,\n",
      "Thre we to the heaves' lovery this mover wiftred\n",
      "To s with miny Pomplany my garise: them tue, buth it,\n",
      "Then you make thest ocrence arcents,\n",
      "To lence downs your fa\n",
      "want offencier. I will clove a for tin or ble day.\n",
      "3 VINIUS:\n",
      "Whenk for she perse deide, not the to shall peep a eashall fit his conqual to thenly.\n",
      "Proferfore for Warwifeth come morterny:\n",
      "Yetch do crouds we many eartay word sanates abarence, ward but smile; you'l thest spe-gech me?\n",
      "\n",
      "Firsegg\n",
      "AN ine Cliffollowinew are soble to\n",
      "MEO:\n",
      "I am in here an tonguit in breate more him: wound her spirmpay all draintispesid;\n",
      "Who ss anst thenfor quite Trongues minusherving\n",
      "As thance--\n",
      "\n",
      "CAMILLO:\n",
      "You soeman:\n",
      "At Rome, and be bosour stray:\n",
      "He well; lord in mand wirgnged sruffor word,\n",
      "May grace shamrovenge she efors burter.\n",
      "\n",
      "Clowinnot if Henry\n",
      "soffice faint thoor to be nist and thy trapss in to thate\n",
      "With hum, and ease I his be therds now,\n",
      "To you blhought conce rongues will ream!--\n",
      "What screfor aim, his Jove 'ty dight;\n",
      "The hell helpher, Cationce fasucr; this santost?\n",
      "I'll thou in not? hearewerefour'for bletch,\n",
      "And ris if it theeps now-fiend o;\n",
      "For worths a prightirs' be ucontleman:\n",
      "Terny it gimolk\n",
      "But onour; stantge bends! Would, ll notwaise,\n",
      "I would Claudinnssold bien oite--\n",
      "Cliffers tirneed, have usking love,\n",
      "And on the ten to pour commorces, theing you secrsters so shough arportern litle dim an him word of wrotnound die;\n",
      "Antence man up your saicle breate;\n",
      "Who willly re him tame, to th\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 2_000\n",
    "seed_text = \". \"\n",
    "seed_idx = torch.tensor([stoi[c] for c in seed_text], device=device).unsqueeze(0)\n",
    "predictions = m.generate(seed_idx, max_new_tokens).to(device)\n",
    "pad_len = m.block_size\n",
    "generated_text = decode(predictions[0].tolist())\n",
    "generated_text = generated_text[pad_len:]  # Remove leading padding\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
