{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "d_model = 12\n",
    "block_size = 6\n",
    "batch_size = 16\n",
    "n_heads = 2\n",
    "dropout = 0.1\n",
    "learning_rate = 1e-3\n",
    "epochs = 10_000\n",
    "eval_iters = 200\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention class\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, d_model,  dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.att_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1).bool())\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q = x\n",
    "        k = x\n",
    "        v = x\n",
    "        B,T,_ = x.shape \n",
    "        dk = d_model // n_heads\n",
    "\n",
    "        # linear projections\n",
    "        q = self.query(q) \n",
    "        k = self.key(k) \n",
    "        v = self.value(v) \n",
    "\n",
    "        # add number of heads\n",
    "        q = q.view(B,T,n_heads,dk).permute(0,2,1,3)   # B,T,h,dk\n",
    "        k = k.view(B,T,n_heads,dk).permute(0,2,1,3)  \n",
    "        v = v.view(B,T,n_heads,dk).permute(0,2,1,3)  \n",
    "        \n",
    "        # attention \n",
    "\n",
    "        x = q @ k.transpose(-2,-1) # B,h,T,dk @ B,h,dk,T --> B,h,T,T\n",
    "        x = x * dk ** -0.5 # B,h,T,T\n",
    "        x = x.masked_fill(self.mask, float('-inf')) # B,h,T,T\n",
    "        x = F.softmax(x, dim=(-1)) # B,n_h,T,T \n",
    "        x = x @ v  # B,h,T,T @ B,T,h,dv --> B,h,T,dv\n",
    "        B,h,T,dv = x.shape\n",
    "        x = x.transpose(2,1).contiguous().view(B,T,h*dv) #B,T,C\n",
    "        out = self.att_proj(x) # B,T,C\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(block_size, d_model)\n",
    "        self.mha = MultiHeadAttention(n_heads, d_model)\n",
    "        # self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, batch_first=True) #PyTorch class for debugging\n",
    "        self.out = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "\n",
    "        embeds = self.embedding_table(x)\n",
    "        positions = self.pos_embedding(torch.arange(self.block_size, device=device))\n",
    "        x = embeds + positions\n",
    "\n",
    "        x = self.mha(x) # b,t,c\n",
    "        logits = self.out(x) # b,t,vocab_size\n",
    "\n",
    "        # # PyTorch mha\n",
    "        # # Create a causal mask of shape (T, T)\n",
    "        \n",
    "        # mask = torch.triu(torch.ones(T, T), diagonal=1).bool().unsqueeze(0)\n",
    "        # mask = mask.repeat(B * n_heads, 1, 1).to(device)\n",
    "        # x, att_scores = self.mha(x, x, x, attn_mask=mask)\n",
    "        # logits = self.out(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = logits.reshape(-1, logits.shape[-1])\n",
    "            targets = targets.reshape(-1)\n",
    "            loss = F.cross_entropy(input=logits, target=targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        B, T = idx.shape\n",
    "        if T < self.block_size:\n",
    "            # pad the input with zeros if it's less than block_size\n",
    "            idx = F.pad(idx, (0, self.block_size - T))\n",
    "        for _ in range(max_new_tokens):\n",
    "            # use only the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3'].\n",
      "Vocab size: 65\n",
      "very dog to the commonalty.\n",
      "\n",
      "Second Citizen:\n",
      "Consi\n",
      "Training samples: 1003854\n",
      "Validation samples: 111539\n"
     ]
    }
   ],
   "source": [
    "data = open('text.txt').read()\n",
    "vocab = list(sorted(set(data)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'Vocab: {vocab[:10]}.')\n",
    "print(f'Vocab size: {vocab_size}')\n",
    "\n",
    "stoi = {c:i for i, c in enumerate(vocab)}\n",
    "itos = {i:c for i, c in enumerate(vocab)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda i: ''.join([itos[i] for i in i])\n",
    "\n",
    "print(decode(encode(data[1100:1150])))\n",
    "data = torch.tensor(encode(data))\n",
    "\n",
    "n_tr = int(len(data) * 0.9)\n",
    "n_val = len(data) - n_tr\n",
    "\n",
    "train = data[:n_tr]\n",
    "val = data[n_tr+1:]\n",
    "\n",
    "print(f'Training samples: {train.shape[0]}')\n",
    "print(f'Validation samples: {val.shape[0]}')\n",
    "\n",
    "def make_batches(split):\n",
    "\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "Xb, Yb = make_batches('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embedding_table): Embedding(65, 12)\n",
      "  (pos_embedding): Embedding(6, 12)\n",
      "  (mha): MultiHeadAttention(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (query): Linear(in_features=12, out_features=12, bias=False)\n",
      "    (key): Linear(in_features=12, out_features=12, bias=False)\n",
      "    (value): Linear(in_features=12, out_features=12, bias=False)\n",
      "    (att_proj): Linear(in_features=12, out_features=12, bias=False)\n",
      "  )\n",
      "  (out): Linear(in_features=12, out_features=65, bias=False)\n",
      ")\n",
      "Total parameters: 2208\n"
     ]
    }
   ],
   "source": [
    "m = Model(vocab_size, block_size).to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "n_params = sum(p.nelement() for p in m.parameters())\n",
    "print(m)\n",
    "print(f'Total parameters: {n_params}')\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(m):\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = make_batches(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 999. Training Loss: 2.844. Evaluation Loss: 2.826\n",
      "Iteration 1999. Training Loss: 2.621. Evaluation Loss: 2.620\n",
      "Iteration 2999. Training Loss: 2.570. Evaluation Loss: 2.579\n",
      "Iteration 3999. Training Loss: 2.536. Evaluation Loss: 2.534\n",
      "Iteration 4999. Training Loss: 2.499. Evaluation Loss: 2.525\n",
      "Iteration 5999. Training Loss: 2.500. Evaluation Loss: 2.506\n",
      "Iteration 6999. Training Loss: 2.496. Evaluation Loss: 2.500\n",
      "Iteration 7999. Training Loss: 2.484. Evaluation Loss: 2.477\n",
      "Iteration 8999. Training Loss: 2.474. Evaluation Loss: 2.461\n",
      "Iteration 9999. Training Loss: 2.481. Evaluation Loss: 2.469\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    Xb, Yb = make_batches('train')\n",
    "    logits, loss = m(Xb, Yb) # B, C\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #break\n",
    "    if epoch % 1000 == 999:\n",
    "        l = estimate_loss(m)\n",
    "        print(f\"Iteration {epoch}. Training Loss: {l['train']:.3f}. Evaluation Loss: {l['val']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, foulathe at bilghind:\n",
      "dy be nekg INI\n",
      "SERRDY:\n",
      "LTI:\n",
      "Aighad peardang;\n",
      "Com wile arr yut Verer, fot'f feiincuen avarsuresthe?\n",
      "\n",
      "Whellat.\n",
      "\n",
      "FOF,\n",
      "Aeet Yonurth oundt the towcay aut.\n",
      "\n",
      "Nhineso, marlded the itisedy tur yofuceve heerl, torurt ye.\n",
      "\n",
      "S:\n",
      "I soit on bou, hanour, batacam portund we Ce wobtpom Ry mecovaclllg mith dreestw rou wator wouevey that at min, ofes ford be hatr nistee thles haarlom\n",
      "hriver heysicseshe theey ny bsh hragene, horf sieirnd astold weaw thish gothth ghor?\n",
      "\n",
      "Whaw mate lael themct, ouomed homathy okighagey theea.\n",
      "PS, Eing thin vap ros pwerald yllghare pruer com;\n",
      "py yererlee-yastint,\n",
      "A'd astou py fir'go cornorae scith th ton ere nollen anldon grous. Mathll lled bit hae.\n",
      "\n",
      "AEKEAROO\n",
      "Wut ou thinclis;\n",
      "Eou kencf Pod me you fons chs\n",
      "Ad tecolederthe min ie de wig:\n",
      "Sure proun'lmwod toth st, yunnd'jo theris hiters Evitornt wain kowu,\n",
      "Feae-tinsy fhe ilr eth yor-ja. Vin? the the jeld bo, ly corld ofu seng urers yous pokcet fan thithl.\n",
      "\n",
      "SOEVHSE\n",
      "HMERUKORDTRpiyou ileledisir't s'ksktunleite chee e thenRr hhirandom?\n",
      "\n",
      "SBOICI.\n",
      "\n",
      "\n",
      "IKice lon heninents led thathinord yot forvernu udlleiris aulecon won.-\n",
      "SALII:\n",
      "Mage Ge milourssestpiceanes thsh con, corecuvearear anly sthin!\n",
      "\n",
      "Is\n",
      "ru as ayer ithind thecuercounu moustherikss nohtmy veldey stangort sthe Ae ame tiy he bas dotilo iops, sor at thes'd touncesk heas turlowAuhever shoth whon oum cors, Ire cor, on esuly he tohene pece haveancont I\n",
      "Ba;\n",
      "Fnd, aicarverete, maghagh faelry\n",
      "Aeas,\n",
      "The bed I in\n",
      "VAo thing!\n",
      "Rindild amy lin cou ethe wea why thif hes hrpu've as I per fohed prit wethavowye inveyith: chanis non; the ith' le hetar yallean lanoos me irath houda uk thecy louwred rou thes thivark tho, suth torend ba'gh wer thar vaf thefar me how bold werece sers ve mokerndoui'lsth me lirot shi yhong shid,\n",
      "Whe aferinde aryinours: ou inve yof whastes proueath ith tor mouln, the the,\n",
      "Habrer KE:\n",
      "I sm dowuftellelidincthe birel anders thyerprron che orinem arstu bel poupi, theourl syve tikep sistheloud ofucy ton ham wad ilasy gas'yj oru end dich bein\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 2_000\n",
    "seed_text = \"Once upon a time, \"\n",
    "seed_idx = torch.tensor([stoi[c] for c in seed_text], device=device).unsqueeze(0)\n",
    "predictions = m.generate(seed_idx, max_new_tokens).to(device)\n",
    "print(decode(predictions[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# DEBUGGING\n",
    "\n",
    "torch.manual_seed(0)\n",
    "X = torch.randn(4,6,2,6)\n",
    "\n",
    "q = X\n",
    "k = X\n",
    "v = X\n",
    "\n",
    "d_k = q.size()[-1]\n",
    "attn_logits = torch.matmul(q, k.transpose(-2, -1)) # B,T,h,dk @ B,T,dk,h > B,T,h,h\n",
    "attn_logits_s = attn_logits * d_k ** -0.5\n",
    "# if mask is not None:\n",
    "#     attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "attention = F.softmax(attn_logits_s, dim=-1)\n",
    "values = torch.matmul(attention, v)\n",
    "\n",
    "##########################\n",
    "\n",
    "att = (q.transpose(1,2) @ k.permute(0,2,3,1)) * d_k ** -0.5 # B,h,T,dk @ B,h,dk,T -> B,h,T,T\n",
    "B,nh,T,_ = att.shape\n",
    "att_soft = F.softmax(att.view(-1,T,T), dim=(-1)).view(B,nh,T,T) # B,h,T,T \n",
    "mvalues = att_soft @ v.permute(0, 2, 1, 3).contiguous()  # B,h,T,T @ B,h,T,dv --> B,h,T,dv\n",
    "mvalues = mvalues.view(B,T,nh,T)\n",
    "\n",
    "#########################\n",
    "\n",
    "mask = torch.triu(torch.ones(2, 2), diagonal=1).bool()\n",
    "a_t = q @ k.transpose(-2,-1) # B,T,h,h\n",
    "a_t_s = a_t * d_k ** -0.5 # B,T,h,h\n",
    "# x = a_t_s.masked_fill(mask, float('-inf')) # B,T,h,h\n",
    "a = F.softmax(a_t_s, -1) # B,T,h,h\n",
    "va = a @ v # B,T,h,h @ B,T,h,dk > B,T,h,dk\n",
    "\n",
    "###########################\n",
    "print(torch.allclose(mvalues, values, atol=1e-5))\n",
    "print(torch.allclose(values, va, atol=1e-4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
