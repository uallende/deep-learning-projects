{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "d_model = 12\n",
    "block_size = 6\n",
    "batch_size = 16\n",
    "n_heads = 2\n",
    "dropout = 0.1\n",
    "learning_rate = 1e-3\n",
    "epochs = 10_000\n",
    "eval_iters = 200\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention class\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, d_model,  dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.att_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1).bool())\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q = x\n",
    "        k = x\n",
    "        v = x\n",
    "        B,T,_ = x.shape \n",
    "        dk = d_model // n_heads\n",
    "\n",
    "        # linear projections\n",
    "        q = self.query(q) \n",
    "        k = self.key(k) \n",
    "        v = self.value(v) \n",
    "\n",
    "        # add number of heads\n",
    "        q = q.view(B,T,n_heads,dk).permute(0,2,1,3)   # B,T,h,dk\n",
    "        k = k.view(B,T,n_heads,dk).permute(0,2,1,3)  \n",
    "        v = v.view(B,T,n_heads,dk).permute(0,2,1,3)  \n",
    "        \n",
    "        # attention \n",
    "\n",
    "        x = q @ k.transpose(-2,-1) # B,h,T,dk @ B,h,dk,T --> B,h,T,T\n",
    "        x = x * dk ** -0.5 # B,h,T,T\n",
    "        x = x.masked_fill(self.mask, float('-inf')) # B,h,T,T\n",
    "        x = F.softmax(x, dim=(-1)) # B,n_h,T,T \n",
    "        x = x @ v  # B,h,T,T @ B,T,h,dv --> B,h,T,dv\n",
    "        B,h,T,dv = x.shape\n",
    "        x = x.transpose(2,1).contiguous().view(B,T,h*dv) #B,T,C\n",
    "        out = self.att_proj(x) # B,T,C\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(block_size, d_model)\n",
    "        self.mha = MultiHeadAttention(n_heads, d_model)\n",
    "        # self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, batch_first=True) #PyTorch class for debugging\n",
    "        self.out = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "\n",
    "        embeds = self.embedding_table(x)\n",
    "        positions = self.pos_embedding(torch.arange(block_size, device=device))\n",
    "        x = embeds + positions\n",
    "\n",
    "        x = self.mha(x) # b,t,c\n",
    "        logits = self.out(x) # b,t,vocab_size\n",
    "\n",
    "        # # PyTorch mha\n",
    "        # # Create a causal mask of shape (T, T)\n",
    "        \n",
    "        # mask = torch.triu(torch.ones(T, T), diagonal=1).bool().unsqueeze(0)\n",
    "        # mask = mask.repeat(B * n_heads, 1, 1).to(device)\n",
    "        # x, att_scores = self.mha(x, x, x, attn_mask=mask)\n",
    "        # logits = self.out(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = logits.reshape(-1, logits.shape[-1])\n",
    "            targets = targets.reshape(-1)\n",
    "            loss = F.cross_entropy(input=logits, target=targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, n_chars, ix):\n",
    "    \n",
    "        for _ in range(n_chars):\n",
    "\n",
    "            logits, loss = self(ix) # B, T, C\n",
    "            logits = logits[:,-1,:] # B, C -- we need to reshape to calculate probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # B, C\n",
    "            next_ix = torch.multinomial(input=probs, num_samples=1)\n",
    "            ix = torch.cat((ix, next_ix), dim=1)\n",
    "\n",
    "        return ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3'].\n",
      "Vocab size: 65\n",
      "very dog to the commonalty.\n",
      "\n",
      "Second Citizen:\n",
      "Consi\n",
      "Training samples: 1003854\n",
      "Validation samples: 111539\n"
     ]
    }
   ],
   "source": [
    "data = open('text.txt').read()\n",
    "vocab = list(sorted(set(data)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'Vocab: {vocab[:10]}.')\n",
    "print(f'Vocab size: {vocab_size}')\n",
    "\n",
    "stoi = {c:i for i, c in enumerate(vocab)}\n",
    "itos = {i:c for i, c in enumerate(vocab)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda i: ''.join([itos[i] for i in i])\n",
    "\n",
    "print(decode(encode(data[1100:1150])))\n",
    "data = torch.tensor(encode(data))\n",
    "\n",
    "n_tr = int(len(data) * 0.9)\n",
    "n_val = len(data) - n_tr\n",
    "\n",
    "train = data[:n_tr]\n",
    "val = data[n_tr+1:]\n",
    "\n",
    "print(f'Training samples: {train.shape[0]}')\n",
    "print(f'Validation samples: {val.shape[0]}')\n",
    "\n",
    "def make_batches(split):\n",
    "\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "Xb, Yb = make_batches('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embedding_table): Embedding(65, 12)\n",
      "  (pos_embedding): Embedding(6, 12)\n",
      "  (mha): MultiHeadAttention(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (query): Linear(in_features=12, out_features=12, bias=False)\n",
      "    (key): Linear(in_features=12, out_features=12, bias=False)\n",
      "    (value): Linear(in_features=12, out_features=12, bias=False)\n",
      "    (att_proj): Linear(in_features=12, out_features=12, bias=False)\n",
      "  )\n",
      "  (out): Linear(in_features=12, out_features=65, bias=False)\n",
      ")\n",
      "Total parameters: 2208\n"
     ]
    }
   ],
   "source": [
    "m = Model(vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "n_params = sum(p.nelement() for p in m.parameters())\n",
    "print(m)\n",
    "print(f'Total parameters: {n_params}')\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(m):\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = make_batches(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 999. Training Loss: 2.423. Evaluation Loss: 2.437\n",
      "Iteration 1999. Training Loss: 2.436. Evaluation Loss: 2.429\n",
      "Iteration 2999. Training Loss: 2.422. Evaluation Loss: 2.425\n",
      "Iteration 3999. Training Loss: 2.416. Evaluation Loss: 2.401\n",
      "Iteration 4999. Training Loss: 2.403. Evaluation Loss: 2.430\n",
      "Iteration 5999. Training Loss: 2.435. Evaluation Loss: 2.398\n",
      "Iteration 6999. Training Loss: 2.409. Evaluation Loss: 2.424\n",
      "Iteration 7999. Training Loss: 2.412. Evaluation Loss: 2.420\n",
      "Iteration 8999. Training Loss: 2.398. Evaluation Loss: 2.420\n",
      "Iteration 9999. Training Loss: 2.408. Evaluation Loss: 2.417\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    Xb, Yb = make_batches('train')\n",
    "    logits, loss = m(Xb, Yb) # B, C\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #break\n",
    "    if epoch % 1000 == 999:\n",
    "        l = estimate_loss(m)\n",
    "        print(f\"Iteration {epoch}. Training Loss: {l['train']:.3f}. Evaluation Loss: {l['val']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# DEBUGGING\n",
    "\n",
    "torch.manual_seed(0)\n",
    "X = torch.randn(4,6,2,6)\n",
    "\n",
    "q = X\n",
    "k = X\n",
    "v = X\n",
    "\n",
    "d_k = q.size()[-1]\n",
    "attn_logits = torch.matmul(q, k.transpose(-2, -1)) # B,T,h,dk @ B,T,dk,h > B,T,h,h\n",
    "attn_logits_s = attn_logits * d_k ** -0.5\n",
    "# if mask is not None:\n",
    "#     attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "attention = F.softmax(attn_logits_s, dim=-1)\n",
    "values = torch.matmul(attention, v)\n",
    "\n",
    "##########################\n",
    "\n",
    "att = (q.transpose(1,2) @ k.permute(0,2,3,1)) * d_k ** -0.5 # B,h,T,dk @ B,h,dk,T -> B,h,T,T\n",
    "B,nh,T,_ = att.shape\n",
    "att_soft = F.softmax(att.view(-1,T,T), dim=(-1)).view(B,nh,T,T) # B,h,T,T \n",
    "mvalues = att_soft @ v.permute(0, 2, 1, 3).contiguous()  # B,h,T,T @ B,h,T,dv --> B,h,T,dv\n",
    "mvalues = mvalues.view(B,T,nh,T)\n",
    "\n",
    "#########################\n",
    "\n",
    "mask = torch.triu(torch.ones(2, 2), diagonal=1).bool()\n",
    "a_t = q @ k.transpose(-2,-1) # B,T,h,h\n",
    "a_t_s = a_t * d_k ** -0.5 # B,T,h,h\n",
    "# x = a_t_s.masked_fill(mask, float('-inf')) # B,T,h,h\n",
    "a = F.softmax(a_t_s, -1) # B,T,h,h\n",
    "va = a @ v # B,T,h,h @ B,T,h,dk > B,T,h,dk\n",
    "\n",
    "###########################\n",
    "print(torch.allclose(mvalues, values, atol=1e-5))\n",
    "print(torch.allclose(values, va, atol=1e-4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
