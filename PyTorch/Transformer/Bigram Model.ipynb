{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work on the bigram model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3'].\n",
      "Vocab size: 65\n",
      "very dog to the commonalty.\n",
      "\n",
      "Second Citizen:\n",
      "Consi\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = open('text.txt').read()\n",
    "vocab = list(sorted(set(data)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'Vocab: {vocab[:10]}.')\n",
    "print(f'Vocab size: {vocab_size}')\n",
    "\n",
    "stoi = {c:i for i, c in enumerate(vocab)}\n",
    "itos = {i:c for i, c in enumerate(vocab)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda i: ''.join([itos[i] for i in i])\n",
    "\n",
    "print(decode(encode(data[1100:1150])))\n",
    "data = torch.tensor(encode(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: torch.Size([1003854])\n",
      "Validation samples: torch.Size([111539])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "batch_size = 32\n",
    "\n",
    "n_tr = int(len(data) * 0.9)\n",
    "n_val = len(data) - n_tr\n",
    "\n",
    "train = data[:n_tr]\n",
    "val = data[n_tr+1:]\n",
    "\n",
    "print(f'Training samples: {train.shape}')\n",
    "print(f'Validation samples: {val.shape}')\n",
    "\n",
    "def make_batches(split):\n",
    "\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb, Yb = make_batches('train')\n",
    "Xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "# create vocab\n",
    "# do vocab size\n",
    "# create training and validation splits\n",
    "# create data loader\n",
    "# create the bi-gram lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "emb_table = nn.Embedding(vocab_size, vocab_size).to(device) # (65, 65)\n",
    "logits = emb_table(Xb) # (32, 8, 65)\n",
    "print(logits.shape)\n",
    "B, T, C = logits.shape\n",
    "logits = logits.view(B*T, C)\n",
    "targets = Yb.view(B*T)\n",
    "loss = F.cross_entropy(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0498,  0.3900,  0.8843, -0.6404, -0.5388, -1.1646, -0.5890, -0.0852,\n",
       "         -0.4918,  0.3321, -0.0126, -0.5153, -0.4854, -0.6589, -0.6387, -0.3921,\n",
       "          2.1584, -0.7153,  0.2292, -1.4770,  1.6679,  0.7794, -0.0394,  0.7323,\n",
       "          0.3798,  0.5197,  0.2711,  0.1370,  0.8722,  1.5015, -1.0303,  1.2152,\n",
       "         -0.3852, -0.2712, -1.2364,  1.2883, -1.7084, -1.1360,  0.0991,  0.3909,\n",
       "          1.1893, -0.3269,  0.0831,  0.6745, -0.4875,  0.9542, -0.7550, -1.1399,\n",
       "         -0.7167, -0.1602,  0.0048, -0.1801, -0.3668,  1.8157,  1.1692, -0.3340,\n",
       "          0.9738,  0.1174,  0.0178, -0.4693, -0.9466, -1.0655,  1.6465, -0.2454,\n",
       "          3.0582], device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " tensor(57, device='cuda:0'))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0], targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(57, device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6533, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define logits and target\n",
    "logits_i = torch.tensor([ 0.0498,  0.3900,  0.8843, -0.6404, -0.5388, -1.1646, -0.5890, -0.0852,\n",
    "         -0.4918,  0.3321, -0.0126, -0.5153, -0.4854, -0.6589, -0.6387, -0.3921,\n",
    "          2.1584, -0.7153,  0.2292, -1.4770,  1.6679,  0.7794, -0.0394,  0.7323,\n",
    "          0.3798,  0.5197,  0.2711,  0.1370,  0.8722,  1.5015, -1.0303,  1.2152,\n",
    "         -0.3852, -0.2712, -1.2364,  1.2883, -1.7084, -1.1360,  0.0991,  0.3909,\n",
    "          1.1893, -0.3269,  0.0831,  0.6745, -0.4875,  0.9542, -0.7550, -1.1399,\n",
    "         -0.7167, -0.1602,  0.0048, -0.1801, -0.3668,  1.8157,  1.1692, -0.3340,\n",
    "          0.9738,  0.1174,  0.0178, -0.4693, -0.9466, -1.0655,  1.6465, -0.2454,\n",
    "          3.0582], requires_grad=True)\n",
    "\n",
    "target_i = torch.tensor(57)\n",
    "\n",
    "# Compute cross entropy loss\n",
    "loss_i = F.cross_entropy(logits_i.unsqueeze(0), target_i.unsqueeze(0))\n",
    "loss_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6533, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define logits and target\n",
    "logits_i = torch.tensor([ 0.0498,  0.3900,  0.8843, -0.6404, -0.5388, -1.1646, -0.5890, -0.0852,\n",
    "         -0.4918,  0.3321, -0.0126, -0.5153, -0.4854, -0.6589, -0.6387, -0.3921,\n",
    "          2.1584, -0.7153,  0.2292, -1.4770,  1.6679,  0.7794, -0.0394,  0.7323,\n",
    "          0.3798,  0.5197,  0.2711,  0.1370,  0.8722,  1.5015, -1.0303,  1.2152,\n",
    "         -0.3852, -0.2712, -1.2364,  1.2883, -1.7084, -1.1360,  0.0991,  0.3909,\n",
    "          1.1893, -0.3269,  0.0831,  0.6745, -0.4875,  0.9542, -0.7550, -1.1399,\n",
    "         -0.7167, -0.1602,  0.0048, -0.1801, -0.3668,  1.8157,  1.1692, -0.3340,\n",
    "          0.9738,  0.1174,  0.0178, -0.4693, -0.9466, -1.0655,  1.6465, -0.2454,\n",
    "          3.0582], requires_grad=True)\n",
    "\n",
    "target_i = torch.tensor(57)\n",
    "\n",
    "# Apply the softmax function to the logits to get probabilities\n",
    "probabilities = torch.softmax(logits_i, dim=0)\n",
    "\n",
    "# Compute the negative log likelihood of the true class\n",
    "nll = -torch.log(probabilities[target_i])\n",
    "nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0089, 0.0125, 0.0205, 0.0045, 0.0049, 0.0026, 0.0047, 0.0078, 0.0052,\n",
       "        0.0118, 0.0084, 0.0051, 0.0052, 0.0044, 0.0045, 0.0057, 0.0734, 0.0041,\n",
       "        0.0107, 0.0019, 0.0449, 0.0185, 0.0081, 0.0176, 0.0124, 0.0143, 0.0111,\n",
       "        0.0097, 0.0203, 0.0380, 0.0030, 0.0286, 0.0058, 0.0065, 0.0025, 0.0307,\n",
       "        0.0015, 0.0027, 0.0094, 0.0125, 0.0278, 0.0061, 0.0092, 0.0166, 0.0052,\n",
       "        0.0220, 0.0040, 0.0027, 0.0041, 0.0072, 0.0085, 0.0071, 0.0059, 0.0521,\n",
       "        0.0273, 0.0061, 0.0224, 0.0095, 0.0086, 0.0053, 0.0033, 0.0029, 0.0440,\n",
       "        0.0066, 0.1804], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7083, device='cuda:0', grad_fn=<UnbindBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8095e+00, -1.1302e+00, -1.2551e-01,  1.4313e+00, -2.7304e+00,\n",
       "         1.3788e+00,  1.0882e+00,  1.2311e+00, -7.3206e-01,  6.4607e-01,\n",
       "        -4.5302e-01, -8.2092e-01, -9.0916e-01, -8.6847e-02,  1.3041e+00,\n",
       "         1.0103e+00, -5.9111e-01, -1.6377e-01, -6.1103e-01,  3.8011e-01,\n",
       "         1.5490e+00, -5.8434e-01,  4.7227e-01,  2.2436e-01,  9.9161e-01,\n",
       "         9.4985e-01, -9.4863e-02, -1.6170e+00, -8.2019e-01, -6.4654e-01,\n",
       "        -1.3319e-01,  7.8813e-03, -1.1176e+00,  6.4368e-01,  1.8809e-01,\n",
       "         7.1894e-01,  1.7083e+00,  1.4248e+00,  1.2541e+00,  1.1685e+00,\n",
       "         6.8397e-01, -4.5269e-01, -1.2468e+00, -8.8542e-01, -1.1985e-01,\n",
       "         7.2042e-01, -3.5219e-01, -2.6869e+00,  1.2400e+00,  6.7754e-01,\n",
       "         1.5006e-01,  4.5240e-01, -1.0221e+00,  6.9845e-01, -6.6867e-02,\n",
       "        -1.3289e-01, -7.6452e-01,  1.3142e+00, -2.6511e-01, -3.3069e-01,\n",
       "         6.2809e-01, -2.0188e-03,  6.8674e-01,  5.9731e-01, -7.0109e-01],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcross_entropy(target\u001b[39m=\u001b[39;49my, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mx)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Float'"
     ]
    }
   ],
   "source": [
    "loss = F.cross_entropy(target=y, input=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __innit__(self, vocab_size):\n",
    "        super().__innit__()\n",
    "\n",
    "        self.embedding_table = nn.Embbeding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, ix, targets=None):\n",
    "\n",
    "        logits = self.embedding_table(ix)\n",
    "\n",
    "        if targets == None:\n",
    "            None\n",
    "        else:\n",
    "            loss = F.cross_entropy(target=targets, input=logits)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
