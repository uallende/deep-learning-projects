{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import sentencepiece as spm\n",
    "\n",
    "block_size = 128\n",
    "batch_size = 32\n",
    "n_heads = 8\n",
    "n_layers = 12\n",
    "d_model = 128+32 * n_heads\n",
    "dff = d_model * 4\n",
    "dropout = 0.8\n",
    "learning_rate = 5e-4\n",
    "epochs = 1_000\n",
    "eval_iters = 20\n",
    "\n",
    "# block_size = 32\n",
    "# batch_size = 32\n",
    "# n_heads = 2\n",
    "# n_layers = 2\n",
    "# d_model = 32 * n_heads\n",
    "# dff = d_model * 4\n",
    "# dropout = 0.6\n",
    "# learning_rate = 5e-4\n",
    "# epochs = 2_000\n",
    "# eval_iters = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# tokenizer = Tokenizer(BPE())\n",
    "# tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"\\n\"], vocab_size=8000)\n",
    "# tokenizer.train(files=[\"text_all.txt\"], trainer=trainer)\n",
    "# tokenizer.save(\"es_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download texts from https://www.gutenberg.org/cache/epub/70219/pg70219.txt from the internet\n",
    "\n",
    "# !wget https://www.gutenberg.org/cache/epub/70219/pg70219.txt\n",
    "# !wget https://www.gutenberg.org/files/50430/50430-0.txt\n",
    "# !wget https://www.gutenberg.org/cache/epub/42727/pg42727.txt\n",
    "# !wget https://www.gutenberg.org/cache/epub/15725/pg15725.txt\n",
    "# !wget https://www.gutenberg.org/files/57654/57654-0.txt\n",
    "# !wget https://www.gutenberg.org/cache/epub/57303/pg57303.txt\n",
    "# !wget https://www.gutenberg.org/cache/epub/47287/pg47287.txt\n",
    "# !wget https://www.gutenberg.org/cache/epub/17013/pg17013.txt\n",
    "\n",
    "# with open('pg70219.txt', 'r') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# with open('50430-0.txt', 'r') as f:\n",
    "#     text += f.read()\n",
    "\n",
    "# with open('pg42727.txt', 'r') as f:\n",
    "#     text += f.read()\n",
    "\n",
    "# with open('pg15725.txt', 'r') as f:\n",
    "#     text += f.read()\n",
    "\n",
    "# with open('57654-0.txt', 'r') as f:\n",
    "#     text += f.read()\n",
    "\n",
    "# with open('pg57303.txt', 'r') as f:\n",
    "#     text += f.read()\n",
    "\n",
    "# book_list = ['pg70219.txt', '50430-0.txt', 'pg42727.txt', 'pg15725.txt', '57654-0.txt', 'pg57303.txt', 'pg47287.txt', 'pg17013.txt']\n",
    "\n",
    "# text = ''\n",
    "# for book in book_list:\n",
    "#     with open(book, 'r') as f:\n",
    "#         text += f.read()\n",
    "\n",
    "# # save text to file\n",
    "# with open('text_all.txt', 'w') as f:\n",
    "#     f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, d_model, block_size, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.att_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size).bool(), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q = x\n",
    "        k = x\n",
    "        v = x\n",
    "        B,T,_ = x.shape \n",
    "        dk = d_model // n_heads\n",
    "\n",
    "        # linear projections\n",
    "        q = self.query(q) \n",
    "        k = self.key(k) \n",
    "        v = self.value(v) \n",
    "\n",
    "        # add number of heads\n",
    "        q = q.view(B,T,n_heads,dk).permute(0,2,1,3)   # B,T,h,dk\n",
    "        k = k.view(B,T,n_heads,dk).permute(0,2,1,3)  \n",
    "        v = v.view(B,T,n_heads,dk).permute(0,2,1,3)  \n",
    "        \n",
    "        # attention \n",
    "        x = q @ k.transpose(-2,-1) # B,h,T,dk @ B,h,dk,T --> B,h,T,T\n",
    "        x = x * dk ** -0.5 # B,h,T,T\n",
    "        x = x.masked_fill(self.mask, float('-inf')) # B,h,T,T\n",
    "        x = F.softmax(x, dim=(-1)) # B,n_h,T,T \n",
    "        x = x @ v  # B,h,T,T @ B,T,h,dv --> B,h,T,dv\n",
    "        B,h,T,dv = x.shape\n",
    "        x = x.transpose(2,1).contiguous().view(B,T,h*dv) #B,T,C\n",
    "        out = self.att_proj(x) # B,T,C\n",
    "\n",
    "        return out\n",
    "    \n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, block_size, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.att = MultiHeadAttention(n_heads, d_model, block_size, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.att(x)\n",
    "        return x\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dff, dropout, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.seq = nn.Sequential(\n",
    "                    nn.Linear(d_model, dff),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(dff, d_model)\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seq(x)\n",
    "        return x\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, block_size, dropout, dff) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.att = AttentionLayer(n_heads, d_model,\n",
    "                                  block_size, dropout)\n",
    "        \n",
    "        self.ffw = FeedForward(d_model, dff, dropout)\n",
    "        self.lnorm1 = nn.LayerNorm(d_model)\n",
    "        self.lnorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.att(self.lnorm1(x))\n",
    "        x = x + self.ffw(self.lnorm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=text_all.txt --model_prefix=m --vocab_size=8000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: text_all.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: text_all.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 108316 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=6588805\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9523% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=109\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999523\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 108316 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=3490843\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 209019 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 108316\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 139038\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 139038 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=68689 obj=11.7361 num_tokens=287710 num_tokens/piece=4.18859\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=60828 obj=9.3977 num_tokens=288959 num_tokens/piece=4.75043\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=45612 obj=9.43305 num_tokens=306771 num_tokens/piece=6.72566\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=45597 obj=9.40264 num_tokens=307368 num_tokens/piece=6.74097\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=34196 obj=9.56834 num_tokens=331795 num_tokens/piece=9.70274\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=34195 obj=9.52931 num_tokens=331906 num_tokens/piece=9.70627\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=25646 obj=9.7343 num_tokens=358868 num_tokens/piece=13.9931\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=25646 obj=9.69282 num_tokens=358863 num_tokens/piece=13.9929\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=19234 obj=9.92251 num_tokens=386834 num_tokens/piece=20.112\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=19234 obj=9.87824 num_tokens=386864 num_tokens/piece=20.1135\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=14425 obj=10.131 num_tokens=413928 num_tokens/piece=28.6952\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=14425 obj=10.0862 num_tokens=413942 num_tokens/piece=28.6962\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=10818 obj=10.3598 num_tokens=439928 num_tokens/piece=40.6663\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=10818 obj=10.3114 num_tokens=439979 num_tokens/piece=40.671\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=10.5187 num_tokens=457447 num_tokens/piece=51.9826\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=10.483 num_tokens=457488 num_tokens/piece=51.9873\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 27476288\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, dropout, dff, n_layers, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(block_size, d_model)\n",
    "\n",
    "        self.decoder = nn.Sequential(*[DecoderLayer(n_heads,\n",
    "                                                    d_model,\n",
    "                                                    block_size,\n",
    "                                                    dropout,\n",
    "                                                    dff) \n",
    "                                                    for _ in range(n_layers)])\n",
    "\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "\n",
    "        embeds = self.embedding_table(x)\n",
    "        positions = self.pos_embedding(torch.arange(block_size, device=device))\n",
    "        x = embeds + positions\n",
    "        x = self.decoder(x)\n",
    "        logits = self.out(x)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(input=logits, target=targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        B, T = idx.shape\n",
    "        if T < self.block_size:\n",
    "            # pad the input with zeros if it's less than block_size\n",
    "            idx = F.pad(idx, (0, self.block_size - T))\n",
    "        for _ in range(max_new_tokens):\n",
    "            # use only the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "spm.SentencePieceTrainer.train('--input=text_all.txt --model_prefix=m --vocab_size=8000')\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "data = open('text_all.txt').read()\n",
    "data_ids = sp.encode_as_ids(data)\n",
    "\n",
    "# Get the vocab\n",
    "vocab = {sp.id_to_piece(id): id for id in range(sp.get_piece_size())}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# stoi = {c:i for i, c in enumerate(vocab)}\n",
    "# itos = {i:c for i, c in enumerate(vocab)}\n",
    "\n",
    "# encode = lambda s: [stoi[c] for c in s]\n",
    "# decode = lambda i: ''.join([itos[i] for i in i])\n",
    "\n",
    "data = torch.tensor(data_ids, dtype=torch.long)\n",
    "\n",
    "n_tr = int(len(data) * 0.9)\n",
    "n_val = len(data) - n_tr\n",
    "\n",
    "train = data[:n_tr]\n",
    "val = data[n_tr+1:]\n",
    "\n",
    "def make_batches(split):\n",
    "\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "Xb, Yb = make_batches('train')\n",
    "m = Model(vocab_size, block_size, dropout, dff, n_layers).to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "n_params = sum(p.nelement() for p in m.parameters())\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "print(f'Number of parameters: {n_params}')\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(m):\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = make_batches(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99. Training Loss: 5.902. Evaluation Loss: 6.029\n",
      "Iteration 199. Training Loss: 5.473. Evaluation Loss: 5.661\n",
      "Iteration 299. Training Loss: 5.148. Evaluation Loss: 5.442\n",
      "Iteration 399. Training Loss: 4.959. Evaluation Loss: 5.264\n",
      "Iteration 499. Training Loss: 4.806. Evaluation Loss: 5.157\n",
      "Iteration 599. Training Loss: 4.669. Evaluation Loss: 5.063\n",
      "Iteration 699. Training Loss: 4.549. Evaluation Loss: 5.013\n",
      "Iteration 799. Training Loss: 4.433. Evaluation Loss: 4.932\n",
      "Iteration 899. Training Loss: 4.317. Evaluation Loss: 4.889\n",
      "Iteration 999. Training Loss: 4.240. Evaluation Loss: 4.893\n",
      "Iteration 1099. Training Loss: 4.189. Evaluation Loss: 4.862\n",
      "Iteration 1199. Training Loss: 4.043. Evaluation Loss: 4.861\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast():\n\u001b[1;32m      5\u001b[0m     logits, loss \u001b[39m=\u001b[39m m(Xb, Yb) \u001b[39m# B, C\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m scaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m      8\u001b[0m scaler\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[1;32m      9\u001b[0m scaler\u001b[39m.\u001b[39mstep(optimizer)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    Xb, Yb = make_batches('train')\n",
    "    \n",
    "    with torch.cuda.amp.autocast():\n",
    "        logits, loss = m(Xb, Yb) # B, C\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    Xb, Yb = make_batches('train')\n",
    "    logits, loss = m(Xb, Yb) # B, C\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 99:\n",
    "        l = estimate_loss(m)\n",
    "        print(f\"Iteration {epoch}. Training Loss: {l['train']:.3f}. Evaluation Loss: {l['val']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un lugar del que no logro acordarme ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ μστ y. Emilio es, compañero de Emilio, y me voiga fué á quien causae un gran ruido, mientras á un agrioso una mirada bajo de hombre. A veces de Udenificame. Al cabo de aquélla subió luego en la boca dentro de la frase de Cosaid, más ungivostórico y en la escala. De esta llamada anrocitaría su empresa que hiciera en padre de estas pisé viviente vocablos, y alguno le traje daré cinco mil pentrio. Lené entre dichosos y les parecias no asentir suelto quieto, como quien se hace al lugar por la suya del temor Telamonio y sólo se abanimeriisé en lo baja arruinismos, no tendrías averiguarción e impusible, sino que enterarnos mal jamás no la intenanas» En San Juan Según ambos el uno de Juanito purificaban de puertas, ascomistas, que cretonabas a París, á orillas del cuyo fin, fué el primero que acababa de retain, podía hallar el ceñidor de la vida. Hizo seguían, que afligiigida por su bondad. Durantes reprimido con respecto á las páginas. Cenestácas y los animaron deviendoal de aquella serie quiso un ingenio, porque los de la cubrido eran eran principios. Me sacó bresco con los curridos con el héroe estos también enciques de sangre con los cañores y la fuerza de Bagdad y los teucros; y aseguría el cadáver de Patroclo; con todas sus acciones, los que eran vencedores separados y ahorliados preparados de su conducta al término fatal enardecldes y una falta de pajas, como si tuvieranlos dando que los arrastran llegar al orden descenderes. Ea, no obstante que la primera empezó entonces les invitó llevarándole la fuerza entre ambos siempre, ni los recabebles podrá sonrbios inPero capellán para expresar con los dioses. Sabas pertenecía, si en esto no hubiesen permanecer, compadeciendo á las mujeres peleas. Meriones, sin falta con temor y comprende para colrtán,[20emos á Patroclo con el sangre.» 111 Dijo, y les arrojó al momento, que los dioses entraron el esfuerzo realiza la pasión y el ligero levantó y negrayó al pecho encima y á Helena, el corcel Patroclo\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, sp, seed_text, max_new_tokens):\n",
    "    seed_token_ids = sp.encode_as_ids(seed_text)\n",
    "    seed_tensor = torch.tensor(seed_token_ids, device=device).unsqueeze(0)\n",
    "    generated_token_ids = model.generate(seed_tensor, max_new_tokens).tolist()[0]\n",
    "    generated_text = sp.decode_ids(generated_token_ids)\n",
    "    return generated_text\n",
    "\n",
    "seed_text = \"En un lugar del que no logro acordarme\"\n",
    "generated_text = generate_text(m, sp, seed_text, 500)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un lugar del que no logro acordarme ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇\n",
      "  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      " ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  \n",
      "⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ μστ y. E\n",
      "milio es, compañero de Emilio, y me voiga fué á quien causae un gran ruido, mientras á un agrioso un\n",
      "a mirada bajo de hombre. A veces de Udenificame. Al cabo de aquélla subió luego en la boca dentro de\n",
      " la frase de Cosaid, más ungivostórico y en la escala. De esta llamada anrocitaría su empresa que hi\n",
      "ciera en padre de estas pisé viviente vocablos, y alguno le traje daré cinco mil pentrio. Lené entre\n",
      " dichosos y les parecias no asentir suelto quieto, como quien se hace al lugar por la suya del temor\n",
      " Telamonio y sólo se abanimeriisé en lo baja arruinismos, no tendrías averiguarción e impusible, sin\n",
      "o que enterarnos mal jamás no la intenanas» En San Juan Según ambos el uno de Juanito purificaban de\n",
      " puertas, ascomistas, que cretonabas a París, á orillas del cuyo fin, fué el primero que acababa de \n",
      "retain, podía hallar el ceñidor de la vida. Hizo seguían, que afligiigida por su bondad. Durantes re\n",
      "primido con respecto á las páginas. Cenestácas y los animaron deviendoal de aquella serie quiso un i\n",
      "ngenio, porque los de la cubrido eran eran principios. Me sacó bresco con los curridos con el héroe \n",
      "estos también enciques de sangre con los cañores y la fuerza de Bagdad y los teucros; y aseguría el \n",
      "cadáver de Patroclo; con todas sus acciones, los que eran vencedores separados y ahorliados preparad\n",
      "os de su conducta al término fatal enardecldes y una falta de pajas, como si tuvieranlos dando que l\n",
      "os arrastran llegar al orden descenderes. Ea, no obstante que la primera empezó entonces les invitó \n",
      "llevarándole la fuerza entre ambos siempre, ni los recabebles podrá sonrbios inPero capellán para ex\n",
      "presar con los dioses. Sabas pertenecía, si en esto no hubiesen permanecer, compadeciendo á las muje\n",
      "res peleas. Meriones, sin falta con temor y comprende para colrtán,[20emos á Patroclo con el sangre.\n",
      "» 111 Dijo, y les arrojó al momento, que los dioses entraron el esfuerzo realiza la pasión y el lige\n",
      "ro levantó y negrayó al pecho encima y á Helena, el corcel Patroclo\n"
     ]
    }
   ],
   "source": [
    "readable = []\n",
    "chunk = 100\n",
    "for i in range(0, len(generated_text), chunk):\n",
    "    readable.append(generated_text[i:i+chunk])\n",
    "\n",
    "print('\\n'.join(readable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
