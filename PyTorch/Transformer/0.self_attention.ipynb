{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 12\n",
    "block_size = 6\n",
    "batch_size = 4\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, ix, targets=None): # ix is the index represented by a B,T,C tensor with character tokens\n",
    "\n",
    "        logits = self.embedding_table(ix)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(input=logits, target=targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, n_chars, ix):\n",
    "        \n",
    "        for _ in range(n_chars):\n",
    "\n",
    "            logits, loss = self(ix) # B, T, C\n",
    "            logits = logits[:,-1,:] # B, C -- we need to reshape to calculate probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # B, C\n",
    "            next_ix = torch.multinomial(input=probs, num_samples=1)\n",
    "            ix = torch.cat((ix, next_ix), dim=1)\n",
    "\n",
    "        return ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size=d_model):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.query = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.value = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        #wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(batch_size, block_size, d_model)\n",
    "tril = torch.tril(torch.ones((block_size, block_size)))\n",
    "\n",
    "q = X\n",
    "k = X\n",
    "v = X\n",
    "\n",
    "m = Head(d_model)\n",
    "y, qu,ke,va = m(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One head attention\n",
    "\n",
    "B,T,C = X.shape\n",
    "\n",
    "query_weights = m.query.weight\n",
    "key_weights = m.key.weight\n",
    "value_weights = m.value.weight\n",
    "\n",
    "q_layer = nn.Linear(d_model, d_model, bias=False)\n",
    "k_layer = nn.Linear(d_model, d_model, bias=False)\n",
    "v_layer = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "q_layer.weight = query_weights\n",
    "k_layer.weight = key_weights\n",
    "v_layer.weight = value_weights\n",
    "\n",
    "Q = q_layer(q) # 4,6,10\n",
    "K = k_layer(k) # 4,6,10\n",
    "V = v_layer(v) # 4,6,10\n",
    "\n",
    "x = (Q @ torch.transpose(K, 1, 2)) * (d_model ** -0.5) # 4,6,10 @ 4,10,6 --> 4, 6, 6\n",
    "x = x.masked_fill(tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "x = F.softmax(x, -1)\n",
    "x = x @ V # 4,6,6 @ 4,6,10 --> 4,6,10\n",
    "\n",
    "# Results are correct\n",
    "torch.all(y == x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3871, 0.6129, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3498, 0.4887, 0.1614, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2544, 0.1407, 0.4314, 0.1735, 0.0000, 0.0000],\n",
      "         [0.1653, 0.4699, 0.1075, 0.1810, 0.0762, 0.0000],\n",
      "         [0.1356, 0.2197, 0.1423, 0.1428, 0.1635, 0.1961]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5592, 0.4408, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2830, 0.4159, 0.3011, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2446, 0.2622, 0.4413, 0.0518, 0.0000, 0.0000],\n",
      "         [0.1133, 0.0324, 0.4285, 0.3145, 0.1114, 0.0000],\n",
      "         [0.1603, 0.1540, 0.2100, 0.1321, 0.1821, 0.1615]]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Multi-head attention\n",
    "\n",
    "n_heads = 2\n",
    "d_model = 10\n",
    "x = torch.randn(4,6,10)\n",
    "\n",
    "q = x\n",
    "k = x\n",
    "v = x\n",
    "\n",
    "B,T,C = x.shape\n",
    "dk = d_model // n_heads\n",
    "dv = d_model // n_heads\n",
    "mask = tril.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "q_layer = nn.Linear(d_model, d_model, bias=False)\n",
    "k_layer = nn.Linear(d_model, d_model, bias=False)\n",
    "v_layer = nn.Linear(d_model, d_model, bias=False)\n",
    "att_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "q = q_layer(q) # 4,6,10\n",
    "k = k_layer(k) # 4,6,10\n",
    "v = v_layer(v) # 4,6,10\n",
    "\n",
    "q = q.view(B,T,n_heads,C//n_heads).permute(0,2,1,3) \n",
    "k = k.view(B,T,n_heads,C//n_heads).permute(0,2,1,3) \n",
    "v = v.view(B,T,n_heads,C//n_heads).permute(0,2,1,3) \n",
    "\n",
    "x = Q @ K.transpose(-2,-1)\n",
    "x = x.masked_fill(mask == 0, float('-inf')) # B,n_h,T,T \n",
    "x = F.softmax(x, dim=(-1)) # B,n_h,T,T \n",
    "x = x @ V # B,n_h,T,T @ B,T,n_h,C//n_h \n",
    "x = x.view(B,T, -1) # B,T,C\n",
    "\n",
    "out = att_proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, d_model, block_size, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.att_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1).bool())\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # initialise as x (would be different for cross-attention)\n",
    "        q = x\n",
    "        k = x\n",
    "        v = x\n",
    "        B,T,C = x.shape\n",
    "        dk = d_model // n_heads\n",
    "        dv = d_model // n_heads\n",
    "\n",
    "        # linear projections\n",
    "        q = self.query(q) \n",
    "        k = self.key(k) \n",
    "        v = self.value(v) \n",
    "\n",
    "        # add number of heads\n",
    "        q = q.view(B,T,n_heads,C//n_heads).permute(0,2,1,3)  \n",
    "        k = k.view(B,T,n_heads,C//n_heads).permute(0,2,1,3) \n",
    "        v = v.view(B,T,n_heads,C//n_heads).permute(0,2,1,3) \n",
    "\n",
    "        # attention \n",
    "        x = q @ k.transpose(-2,-1)\n",
    "        x = x * dk ** -0.5 # B,n_h,T,C @ B,n_h,C,T --> B,n_h,T,T\n",
    "        x = x.masked_fill(self.mask == 0, float('-inf')) # B,n_h,T,T\n",
    "        x = F.softmax(x, dim=(-1)) # B,n_h,T,T \n",
    "        x = x @ v  # B,n_h,T,T @ B,T,n_h,C//n_h \n",
    "        x = x.view(B,T, -1) # B,T,C\n",
    "        out = self.att_proj(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention robust to any n_head\n",
    "# Padding will be added to the input tensor\n",
    "\n",
    "# calculate the number of padding required\n",
    "# add padding to input tensor - it must be 0s\n",
    "\n",
    "d_model = 10\n",
    "n_heads = 3\n",
    "\n",
    "def calc_padding(d_model, n_heads):\n",
    "\n",
    "    if d_model % n_heads != 0:\n",
    "        pad = (n_heads * (d_model//n_heads+1)) - d_model\n",
    "    else:\n",
    "        0\n",
    "\n",
    "    return pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "t4d = torch.empty(3, 3, 4, 2)\n",
    "p1d = (1, 1) # pad last dim by 1 on each side\n",
    "out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Do\n",
    "# Attention - Masked self attention\n",
    "# input tensor B,T,C\n",
    "# q, k, v projections - linear layers with no bias or activation function\n",
    "# get projections by passing q,k,v through layers\n",
    "# attention scores - q @ k where the mask is applied\n",
    "# att_scores @ v\n",
    "# create multi-head attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
