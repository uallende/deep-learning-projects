{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "batch_size = 32\n",
    "epochs = 4000\n",
    "eval_iters = 300\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "learning_rate = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, ix, targets=None): # ix is the index represented by a B,T,C tensor with character tokens\n",
    "\n",
    "        logits = self.embedding_table(ix)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(input=logits, target=targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, n_chars, ix):\n",
    "        \n",
    "        for _ in range(n_chars):\n",
    "\n",
    "            logits, loss = self(ix) # B, T, C\n",
    "            logits = logits[:,-1,:] # B, C -- we need to reshape to calculate probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # B, C\n",
    "            next_ix = torch.multinomial(input=probs, num_samples=1)\n",
    "            ix = torch.cat((ix, next_ix), dim=1)\n",
    "\n",
    "        return ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3'].\n",
      "Vocab size: 65\n",
      "very dog to the commonalty.\n",
      "\n",
      "Second Citizen:\n",
      "Consi\n"
     ]
    }
   ],
   "source": [
    "data = open('text.txt').read()\n",
    "vocab = list(sorted(set(data)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'Vocab: {vocab[:10]}.')\n",
    "print(f'Vocab size: {vocab_size}')\n",
    "\n",
    "stoi = {c:i for i, c in enumerate(vocab)}\n",
    "itos = {i:c for i, c in enumerate(vocab)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda i: ''.join([itos[i] for i in i])\n",
    "\n",
    "print(decode(encode(data[1100:1150])))\n",
    "data = torch.tensor(encode(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1003854\n",
      "Validation samples: 111539\n"
     ]
    }
   ],
   "source": [
    "n_tr = int(len(data) * 0.9)\n",
    "n_val = len(data) - n_tr\n",
    "\n",
    "train = data[:n_tr]\n",
    "val = data[n_tr+1:]\n",
    "\n",
    "print(f'Training samples: {train.shape[0]}')\n",
    "print(f'Validation samples: {val.shape[0]}')\n",
    "\n",
    "def make_batches(split):\n",
    "\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "# create vocab\n",
    "# do vocab size\n",
    "# create training and validation splits\n",
    "# create data loader\n",
    "# create the bi-gram lookup table\n",
    "# train and predict with the Bigram. \n",
    "# print validation during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy - Torch vs. Manual calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input with embeddings: torch.Size([32, 8, 65])\n",
      "Are both calculations identical: 4.177806, 4.177806\n"
     ]
    }
   ],
   "source": [
    "# Learn how cross entropy works.\n",
    "# cross_entropy method vs manual way\n",
    "\n",
    "Xb, Yb = make_batches('train')\n",
    "\n",
    "emb_table = nn.Embedding(vocab_size, vocab_size).to(device) # (65, 65)\n",
    "logits = emb_table(Xb) # (32, 8, 65)\n",
    "print(f'Input with embeddings: {logits.shape}')\n",
    "B, T, C = logits.shape\n",
    "logits = logits.view(B*T, C)\n",
    "targets = Yb.view(B*T)\n",
    "loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "logits_i = logits[0]\n",
    "target_i = targets[0]\n",
    "\n",
    "loss_i = F.cross_entropy(logits_i.unsqueeze(0), target_i.unsqueeze(0))\n",
    "# ------------------------------------------------------------#\n",
    "probabilities = torch.softmax(logits_i, dim=0)\n",
    "nll = -torch.log(probabilities[target_i])\n",
    "print(f'Are both calculations identical: {loss_i.item():.6f}, {nll.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameteres: 4225\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "m = BigramLanguageModel(vocab_size=vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "n_params = sum(p.nelement() for p in m.parameters())\n",
    "print(f'Total parameteres: {n_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(m):\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = make_batches(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 499. Training Loss: 2.944. Evaluation Loss: 2.964\n",
      "Iteration 999. Training Loss: 2.589. Evaluation Loss: 2.600\n",
      "Iteration 1499. Training Loss: 2.514. Evaluation Loss: 2.533\n",
      "Iteration 1999. Training Loss: 2.491. Evaluation Loss: 2.497\n",
      "Iteration 2499. Training Loss: 2.478. Evaluation Loss: 2.489\n",
      "Iteration 2999. Training Loss: 2.475. Evaluation Loss: 2.489\n",
      "Iteration 3499. Training Loss: 2.469. Evaluation Loss: 2.491\n",
      "Iteration 3999. Training Loss: 2.463. Evaluation Loss: 2.486\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    Xb, Yb = make_batches('train')\n",
    "    logits, loss = m(Xb, Yb) # B, C\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 499:\n",
    "        l = estimate_loss(m)\n",
    "        print(f\"Iteration {epoch}. Training Loss: {l['train']:.3f}. Evaluation Loss: {l['val']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LEx\n",
      "\n",
      "TCKICant,\n",
      "Thavet ho If t InavareUKI t, Gore forler s m!\n",
      "3 gs an, t s't.\n",
      "Angonchem cin s the gowhe ht whould f lo d, t gakur pldousathewir han\n",
      "MESen , ayowid:\n",
      "Annoucheathant\n",
      "M:\n",
      "POF IORDUTElNRoug, fl Ang pon is anlave ns tsthere sort vere mane theanaiuthe ad the pe weckithe g?\n",
      "BENe.\n",
      "Jor no, thort, malise Homyo II gheno as\n",
      "DUETHe,\n",
      "AUK:\n",
      "SS:\n",
      "T: ile mspo Von it ugh ivis, t f the s stre wree trton thor averldomedis cin.\n",
      "An. n,\n",
      "We d men ro finy cke ySove theity itheve h de seri-stheshathes hell kea\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros(1,1).int().to(device)\n",
    "predictions = m.generate(n_chars=500, ix=context)\n",
    "print(decode(predictions[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
