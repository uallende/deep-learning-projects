{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m functional \u001b[39mas\u001b[39;00m F\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "block_size = 128\n",
    "batch_size = 128\n",
    "n_heads = 6\n",
    "n_layers = 6\n",
    "d_model = 128 * n_heads\n",
    "dff = d_model * 2.5\n",
    "dropout = 0.2\n",
    "learning_rate = 3e-4\n",
    "epochs = 5_000\n",
    "eval_iters = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download texts from https://www.gutenberg.org/cache/epub/70219/pg70219.txt from the internet\n",
    "\n",
    "# !wget https://www.gutenberg.org/cache/epub/70219/pg70219.txt\n",
    "# !wget https://www.gutenberg.org/files/50430/50430-0.txt\n",
    "# !wget https://www.gutenberg.org/cache/epub/42727/pg42727.txt\n",
    "# !wget https://www.gutenberg.org/cache/epub/15725/pg15725.txt\n",
    "# !wget https://www.gutenberg.org/files/57654/57654-0.txt\n",
    "# !wget https://www.gutenberg.org/cache/epub/57303/pg57303.txt\n",
    "# !wget https://www.gutenberg.org/cache/epub/47287/pg47287.txt\n",
    "# !wget https://www.gutenberg.org/cache/epub/17013/pg17013.txt\n",
    "\n",
    "# with open('pg70219.txt', 'r') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# with open('50430-0.txt', 'r') as f:\n",
    "#     text += f.read()\n",
    "\n",
    "# with open('pg42727.txt', 'r') as f:\n",
    "#     text += f.read()\n",
    "\n",
    "# with open('pg15725.txt', 'r') as f:\n",
    "#     text += f.read()\n",
    "\n",
    "# with open('57654-0.txt', 'r') as f:\n",
    "#     text += f.read()\n",
    "\n",
    "# with open('pg57303.txt', 'r') as f:\n",
    "#     text += f.read()\n",
    "\n",
    "# book_list = ['pg70219.txt', '50430-0.txt', 'pg42727.txt', 'pg15725.txt', '57654-0.txt', 'pg57303.txt', 'pg47287.txt', 'pg17013.txt']\n",
    "\n",
    "# text = ''\n",
    "# for book in book_list:\n",
    "#     with open(book, 'r') as f:\n",
    "#         text += f.read()\n",
    "\n",
    "# # save text to file\n",
    "# with open('text_all.txt', 'w') as f:\n",
    "#     f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, d_model, block_size, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.att_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size).bool(), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q = x\n",
    "        k = x\n",
    "        v = x\n",
    "        B,T,_ = x.shape \n",
    "        dk = d_model // n_heads\n",
    "\n",
    "        # linear projections\n",
    "        q = self.query(q) \n",
    "        k = self.key(k) \n",
    "        v = self.value(v) \n",
    "\n",
    "        # add number of heads\n",
    "        q = q.view(B,T,n_heads,dk).permute(0,2,1,3)   # B,T,h,dk\n",
    "        k = k.view(B,T,n_heads,dk).permute(0,2,1,3)  \n",
    "        v = v.view(B,T,n_heads,dk).permute(0,2,1,3)  \n",
    "        \n",
    "        # attention \n",
    "        x = q @ k.transpose(-2,-1) # B,h,T,dk @ B,h,dk,T --> B,h,T,T\n",
    "        x = x * dk ** -0.5 # B,h,T,T\n",
    "        x = x.masked_fill(self.mask, float('-inf')) # B,h,T,T\n",
    "        x = F.softmax(x, dim=(-1)) # B,n_h,T,T \n",
    "        x = x @ v  # B,h,T,T @ B,T,h,dv --> B,h,T,dv\n",
    "        B,h,T,dv = x.shape\n",
    "        x = x.transpose(2,1).contiguous().view(B,T,h*dv) #B,T,C\n",
    "        out = self.att_proj(x) # B,T,C\n",
    "\n",
    "        return out\n",
    "    \n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, block_size, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.att = MultiHeadAttention(n_heads, d_model, block_size, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.att(x)\n",
    "        return x\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dff, dropout, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.seq = nn.Sequential(\n",
    "                    nn.Linear(d_model, dff),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(dff, d_model)\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seq(x)\n",
    "        return x\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, block_size, dropout, dff) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.att = AttentionLayer(n_heads, d_model,\n",
    "                                  block_size, dropout)\n",
    "        \n",
    "        self.ffw = FeedForward(d_model, dff, dropout)\n",
    "        self.lnorm1 = nn.LayerNorm(d_model)\n",
    "        self.lnorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.att(self.lnorm1(x))\n",
    "        x = x + self.ffw(self.lnorm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 42968299\n",
      "Iteration 99. Training Loss: 2.451. Evaluation Loss: 2.429\n",
      "Iteration 199. Training Loss: 2.134. Evaluation Loss: 2.126\n",
      "Iteration 299. Training Loss: 1.996. Evaluation Loss: 1.983\n",
      "Iteration 399. Training Loss: 1.864. Evaluation Loss: 1.848\n",
      "Iteration 499. Training Loss: 1.756. Evaluation Loss: 1.749\n",
      "Iteration 599. Training Loss: 1.662. Evaluation Loss: 1.669\n",
      "Iteration 699. Training Loss: 1.614. Evaluation Loss: 1.617\n",
      "Iteration 799. Training Loss: 1.558. Evaluation Loss: 1.567\n",
      "Iteration 899. Training Loss: 1.523. Evaluation Loss: 1.535\n",
      "Iteration 999. Training Loss: 1.484. Evaluation Loss: 1.510\n",
      "Iteration 1099. Training Loss: 1.457. Evaluation Loss: 1.485\n",
      "Iteration 1199. Training Loss: 1.432. Evaluation Loss: 1.457\n",
      "Iteration 1299. Training Loss: 1.413. Evaluation Loss: 1.452\n",
      "Iteration 1399. Training Loss: 1.400. Evaluation Loss: 1.432\n",
      "Iteration 1499. Training Loss: 1.385. Evaluation Loss: 1.421\n",
      "Iteration 1599. Training Loss: 1.356. Evaluation Loss: 1.403\n",
      "Iteration 1699. Training Loss: 1.347. Evaluation Loss: 1.393\n",
      "Iteration 1799. Training Loss: 1.326. Evaluation Loss: 1.385\n",
      "Iteration 1899. Training Loss: 1.324. Evaluation Loss: 1.371\n",
      "Iteration 1999. Training Loss: 1.311. Evaluation Loss: 1.371\n",
      "Iteration 2099. Training Loss: 1.294. Evaluation Loss: 1.357\n",
      "Iteration 2199. Training Loss: 1.286. Evaluation Loss: 1.350\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m logits, loss \u001b[39m=\u001b[39m m(Xb, Yb) \u001b[39m# B, C\u001b[39;00m\n\u001b[1;32m    112\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 113\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    114\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m99\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, dropout, dff, n_layers, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(block_size, d_model)\n",
    "\n",
    "        self.decoder = nn.Sequential(*[DecoderLayer(n_heads,\n",
    "                                                    d_model,\n",
    "                                                    block_size,\n",
    "                                                    dropout,\n",
    "                                                    dff) \n",
    "                                                    for _ in range(n_layers)])\n",
    "\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "\n",
    "        embeds = self.embedding_table(x)\n",
    "        positions = self.pos_embedding(torch.arange(block_size, device=device))\n",
    "        x = embeds + positions\n",
    "        x = self.decoder(x)\n",
    "        logits = self.out(x)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(input=logits, target=targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        B, T = idx.shape\n",
    "        if T < self.block_size:\n",
    "            # pad the input with zeros if it's less than block_size\n",
    "            idx = F.pad(idx, (0, self.block_size - T))\n",
    "        for _ in range(max_new_tokens):\n",
    "            # use only the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "data = open('text_all.txt').read()\n",
    "vocab = list(sorted(set(data)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "stoi = {c:i for i, c in enumerate(vocab)}\n",
    "itos = {i:c for i, c in enumerate(vocab)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda i: ''.join([itos[i] for i in i])\n",
    "\n",
    "data = torch.tensor(encode(data))\n",
    "\n",
    "n_tr = int(len(data) * 0.9)\n",
    "n_val = len(data) - n_tr\n",
    "\n",
    "train = data[:n_tr]\n",
    "val = data[n_tr+1:]\n",
    "\n",
    "def make_batches(split):\n",
    "\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "Xb, Yb = make_batches('train')\n",
    "m = Model(vocab_size, block_size, dropout, dff, n_layers).to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "n_params = sum(p.nelement() for p in m.parameters())\n",
    "print(f'Number of parameters: {n_params}')\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(m):\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = make_batches(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    Xb, Yb = make_batches('train')\n",
    "    logits, loss = m(Xb, Yb) # B, C\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 99:\n",
    "        l = estimate_loss(m)\n",
    "        print(f\"Iteration {epoch}. Training Loss: {l['train']:.3f}. Evaluation Loss: {l['val']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--Es pies el que, si si nos trabaja la tierra, end\n",
      "someter un hombre. Este te caías vigor, que tomé lo hicemente\n",
      "     ellos, y todos que traición no se ha quitarado de sías... De\n",
      "     casabas, que no puedo hacer, sin vergüente, ni otros.\n",
      "\n",
      "Las names exhalando de unas montes envinanzasenas, atraelando\n",
      "     fresquen bastantes son en el suelo, donde por poco traemo reconocír\n",
      "     derributón; también así yo la pesadía no hemos encagerme,\n",
      "     apareciere engañar volverías.\n",
      "\n",
      "[Lasteras no sabían que allí nunca encontrémamos en el principio\n",
      "     que unos lechamos están en Occidente.\n",
      "\n",
      "[25] --¡Que tenías indóvil... sintió un gran pedazo--le he conciliado a\n",
      "     cercano y a la temple, nacido de ley que\n",
      "     le da mar, si quieres veces acbedo un peso. Algunos que tenían\n",
      "siempres vasias. Morenos le sentalesen alzar, atravesando bajo\n",
      "      gratemente azotaparroquio te importa. Su hora cuerpo a toda él\n",
      "     se conservaía en su mano a las casas de narrar paer pronunciar\n",
      "      Cube bien. RiPario sabe en redondos porquerías\n",
      "     verle, 24. Luego en bien alto principio la llanura miserables\n",
      "[5]  por su interés ni cuarti andar traían con la tierra luz, en\n",
      "     _Silio se calen una bórrona como cinco dinaria y acertaba ando\n",
      "     templo, II, 17; recogen á quien la Neinera con toda una forma de\n",
      "     Reyno. Aparecióles bien mejor de comer manos de beneficion al\n",
      "     demás, haciendo la corriente atapá.\n",
      "\n",
      "     --Ayer tan inquietuo desir que no hace piel. Esclavos\n",
      "     firma.\n",
      "\n",
      "    --Pero si tuviera se comense con apacer--replica y hermosé aquel\n",
      "     Perú... nada».\n",
      "\n",
      "[10] --Ahora estás Mejicanos.\n",
      "\n",
      "     Y miramos conocidos--dijo él entraron una de las vocando\n",
      "     desgraciadamente.\n",
      "\n",
      "     --Eso es que me viene despléndido que turbaba si viera mejor en\n",
      "     tiempo--replicó el río.--Traen por el atentarse con media\n",
      "[15] historia por otras a ti.\n",
      "\n",
      "     --Pero ¿por qué cierras?\n",
      "\n",
      "     --Que la oír de Primera yo.\n",
      "\n",
      "     --Ra usted Sacaindo un buen acogible cua dicha.--¿Ésta\n",
      "     parecer?--dijo la fachacilidad la llamada.\n",
      "\n",
      "[2] Los que hombres a escorreron los Escritores prójimos.\n",
      "     Las cosecientes es unos remediquios, que el ligage de beber que\n",
      "     decentemrentes secretas._\n",
      "\n",
      "\n",
      "En gabinaco yentas sequenales se licen, y sin ocupar y han cercano\n",
      "Diocoeo con que algunos naturales? ¿Verdad que eres vino a Meranor, en\n",
      "consequiano? Quedo si no se hay tú. ¡Bañero, ¡ay!... con muy provecioso...\n",
      "le ha puesto el vencidor de los Sacrimentos. Al sabio dulce, atacón\n",
      "los Cuzcopos ni con su lamo. Me amenacióse incluírible negro, que el\n",
      "Sol habsta en el Perú, encierto no ferrumería más de ordinario, nadie me\n",
      "difícilmente legarmentos bácas. Las palabras manos gozosos, pero el\n",
      "derramachón a damente, despediendo, tenian puerta, y, que unos un garcoles\n",
      "suyos y sus intensas. Dan focidos una lángul unos de sus géneros, dicen\n",
      "que en medio de estos todos higres: de los mártiles moleros. En no\n",
      "sobrina de ἀρσσ; mata mucha; era algún fresco mundo á Criseida. Hacía\n",
      "enrames en los hombres en la Flecra de Hermos, que iritados acababo\n",
      "diciendo en la cierra, para aprocarme en la establecida. Ea fronda\n",
      "le hacen abnegir el primer charco. Las liciones arcaban barriba, y aire\n",
      "por el caarriciacido de soltar animas en lectora de estas almoracias de\n",
      "ida.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Iituatro tragently.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Ilustración]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_CANTO USTEVIEN\n",
      "\n",
      "\n",
      "El Estreclaremente, que sólo a hacerte en eficallez insición, se Mayor\n",
      "conciertamente frío.\n",
      "\n",
      "Concluídos no se casi muscan en severicum Teners gemeridable arcos de\n",
      "mi rayeto cuatro sinestias, quiénes fuera, arrastro de la Ptía en fríos\n",
      "durante el tiempo. Aunque en el Perú se puso entró sin el oír la dicha\n",
      "y de sentir quebrandas las puertas, ya cosas unas de provechacer\n",
      "ballogosos. Sacó con su cascarona batallas, y reciban á sus poderos de\n",
      "Energ-bacóle el Centero Cómo por embajador de Eleo. Algún banquecilla, que cayó\n",
      "encomparcían con las faltas. Un sembraríle dos meunerarse como si tendráis,\n",
      "mas pesado. Su cama, alejande y miraba en el corillo con toda sus\n",
      "bienes más libertad de conasila y me enteraremos alegre... no tenía\n",
      "como entre malesticipata la conmorcialidad de muchos, chicos, acordó\n",
      "incretilmente, benditen y sin traerse, tomando al obrevluencia las\n",
      "letras de roca voracerted.\n",
      "\n",
      "--¡Oh conocer Dios!--murmuraba alma que del mulquer.\n",
      "\n",
      "Fortunata con atención vivá si \n",
      "puedo mi vida. ¿Ve un hermano a aquí me volví... A ricontes, mata á no\n",
      "deje-María me encuentes.\n",
      "\n",
      "Y si quieres tenecer a comer, tiene la tener acerca de prevesingas y\n",
      "fingituras que, aprendiendo decirse, porque la superioridad de la alcanza\n",
      "que te acordes, en todas las bañanas, las tenemos dentro de la extrama\n",
      "dichosas que no creo. Bolóbales.\n",
      "\n",
      "Cuando entraron en aquella tertura a que le decía: «Otro hombre, hija, oh\n",
      "recreabamente inmenso al pie de la señorita, y encontraran la Dala usarían la\n",
      "caloria, uno de comer. Y nosotros nosotros riqueza de tal modo concluymos,\n",
      "de la cuarenta voz sean ponermes sin estos avisos.\n",
      "Hay tacilles, señóricas, que se irían unos venerables y amastan mal en\n",
      "contrario, que el carro con peloto vegan aquéllos, como podría salir\n",
      "reconocerse en que hacen en las primeras de Mérito careciéndose?\n",
      "\n",
      "--_¿No usted si hay esplighorrini, ver a mi intiente elevando una iña\n",
      "     cañana...._ es ellos sotogros.\n",
      "\n",
      "\n",
      "\n",
      "                                      CAPÍTULO XXXXV\n",
      "\n",
      "                    _De la tierra que con una leantera frente._\n",
      "\n",
      "                                  CAPÍTURO\n",
      "\n",
      "      Dames como cierto o derrictemente.\n",
      "\n",
      "      [199 =erogar prontedes raderastras la extremente:\n",
      "\n",
      "     9. Echad explanation, mectiendes y purificados a celigrarse densarte baral, icentinó\n",
      "     trapoco yo un bleno contraría. Y cuando él morió la sencillez,\n",
      "     dormiréis, sombríaronsela trosártelaronas son inquietud, bocarando\n",
      "     semejantes    de la coca en medio. «Si me esquieres quiero de\n",
      "      emperarmentes... ¿Pero esto, tú, que erá más no le ha visto\n",
      "     que dormir un poco en el estanquitaities de esta malquimiento...\n",
      "     Por proaceré en este menos desmestira venir a\n",
      "     querir.\n",
      "\n",
      "     --En los Guilaites puede serpiración--exclamó Juanito que mi mujer examinada\n",
      "     que mantecamos a Dande.--Este usted».\n",
      "[81] Sigamos una mucha granujera tanto tempral, y quiero otra que\n",
      "     Maestro. Pero, en quien comenda en cuantos años.\n",
      "\n",
      "     --Pues decir más que mi dueño que lo cuente misterio poco\n",
      "ollar para dejar licturas, ¡azca! y\n",
      "     no hay otras crimen condenadas de las acciones. Rosarito y el\n",
      "     adjurador de la muerte a poseerme y después de ello hubo      *\n",
      "\n",
      "Eventer que le ven a cien demente me ven verbelSo ordenónidamente. Esta\n",
      "     dónde, cuando entenderá humor; pues yendo el que lleva, le pudieran\n",
      "me tengo hombre de avanzar en velto, y que no sea muy continela en Seo.\n",
      "\n",
      "Se había en así él diciendo en sus caras; pendiéndome el día y en seguida. Dechase con modos\n",
      "en menester de aquellos gloriaba en el tempero Barbarita termen[dida, en\n",
      "bien la ausencia abismo sin sazonar conmiseras que aplataban, besosíves,\n",
      "irritados, estaban en la calle; el principal, y al llegaba mana Opna, de\n",
      "perseguir la raza; y el hombre conviene en casa de Teates cuales la\n",
      "Ceremesca advirtiéndose. En el cadáver de la A.lio.\n",
      "Acaronmé dijes solo se podían varse á Helenos de la letra.\n",
      "\n",
      "124 Luego, tenían la muerte pobrezca y ser mi Silo, y la que vencion\n",
      "cerrió con el combate y la construcción venibles se recibió al comedor de\n",
      "pensar de fuerza.\n",
      "\n",
      "Acabóse en las naciones de los corderos de entender por Encarnación\n",
      "y Héctor cuyo siguiente era de cualquier edicionales. De este detnativo de\n",
      "maremos á la emisión ídolo atravesado por la clave. Juavi, lengatura,\n",
      "que todo en la insesa, si me ganeciora enarme, y la mejor ejercían a\n",
      "fuerzas de su escenida y extraordinario les trata y de la gorra, y han\n",
      "gran silido de Elpa, que hemos dicho, y sus hermanos sobre un poco y\n",
      "sentirse le digan, que la idea suerte en las Islas legítimas en incomparable\n",
      "Marte. En canto un ratontría interés puebles, que tenía preferir, no\n",
      "tales se les contestó, habiendo enfermo.\n",
      "\n",
      "--¡Oh!... --Digan, mudémemente la mirable--pensó en casi me ha absorto ha habido\n",
      "derritado.\n",
      "\n",
      "Voseribía Parga, mañana, muy hermoso en su propila y no ocimentaba sin\n",
      "tiempo que oprendaba contra en el umna de la catella guerra a las\n",
      "preginas, de la guerra queria habia más ni siquiería ni contestó: otras\n",
      "mirandas, prevando en fabricantes, aunque con amonestaciones de que Argos\n",
      "se acercaban a ponerse el dolor, que eran mas nesorables; los encontraron\n",
      "masas, y las veleran del ver que entre mi hija, porque no siempre me\n",
      "volvieran largas algarmentas eran. Se encuentra antes mas alabardados,\n",
      "doña Amaristo por el médico; y no sabemos ninguna va estos nieblas, y\n",
      "ni la mayorada creciente, aunque no tenía la gravedad y almas lo que\n",
      "chico exeraba el _Ppeteso__.\n",
      "\n",
      "XXIS (que Aposentaba París y, los amarillos Á la historia?\n",
      "\n",
      "Despues su carida, y Estépañelo que viene por echar mi visib sobrino. Or\n",
      "viudo antes se ponian en las espaldas. Mas así, hablaréese antormó la\n",
      "Matana, y se contempló de los Mejicanos.\n",
      "\n",
      "Entonces en el sensimismo, inventimóse (coleidealidity). Ande agita exhortó a\n",
      "Maca, Eneas y Nobles le indignaba el gusto de hombres, salió los\n",
      "granses eritos para recreaban los aetraves de la expresaba en celosa.\n",
      "Vosotros de concuencia: descarataron la ensisteración humana, y como la\n",
      "fórmula Aftigua, doña Lupe de haber aquerido de ser contrado. El\n",
      "Sarpiente, de Cdao, siempre teucros tanto, que no eran a mi puesto... traía más\n",
      "cargar y no tener nada nubes años abraron y liveración en todo el\n",
      "Señor principal, y tienen donde menos fueron el asesino y no sasten,\n",
      "Anantes. Entraba sostenerse con él.\n",
      "\n",
      "531 El desierto que deben efectoses de los gritos, Tizocatorio de covidia\n",
      "le echó á person las barbas este juez, á los griegos, hombres de\n",
      "los vientos se traía, y bandejando á la comunca preferencia. Eran el\n",
      "clavo nofesor de gente hacer chuca», dicen muy cuerpo, así es. Así temor,\n",
      "ármastaron muertos, y no lo perdones aceptar la muerte mente, entre\n",
      "una almorada y. Nunca sobre la occidencia le dejó el catálco.\n",
      "El extremedor de Potosí, \n"
     ]
    }
   ],
   "source": [
    "max_new_tokens =10_000\n",
    "seed_text = \". \"\n",
    "seed_idx = torch.tensor([stoi[c] for c in seed_text], device=device).unsqueeze(0)\n",
    "predictions = m.generate(seed_idx, max_new_tokens).to(device)\n",
    "pad_len = m.block_size\n",
    "generated_text = decode(predictions[0].tolist())\n",
    "generated_text = generated_text[pad_len:]  # Remove leading padding\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
