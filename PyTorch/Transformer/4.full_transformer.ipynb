{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "d_model = 512\n",
    "block_size = 256\n",
    "batch_size = 32 \n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "dff = d_model * 4\n",
    "dropout = 0.2\n",
    "learning_rate = 3e-4\n",
    "epochs =5_000\n",
    "eval_iters = 200\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3'].\n",
      "Vocab size: 65\n",
      "very dog to the commonalty.\n",
      "\n",
      "Second Citizen:\n",
      "Consi\n",
      "Training samples: 1003854\n",
      "Validation samples: 111539\n"
     ]
    }
   ],
   "source": [
    "data = open('text.txt').read()\n",
    "vocab = list(sorted(set(data)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'Vocab: {vocab[:10]}.')\n",
    "print(f'Vocab size: {vocab_size}')\n",
    "\n",
    "stoi = {c:i for i, c in enumerate(vocab)}\n",
    "itos = {i:c for i, c in enumerate(vocab)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda i: ''.join([itos[i] for i in i])\n",
    "\n",
    "print(decode(encode(data[1100:1150])))\n",
    "data = torch.tensor(encode(data))\n",
    "\n",
    "n_tr = int(len(data) * 0.9)\n",
    "n_val = len(data) - n_tr\n",
    "\n",
    "train = data[:n_tr]\n",
    "val = data[n_tr+1:]\n",
    "\n",
    "print(f'Training samples: {train.shape[0]}')\n",
    "print(f'Validation samples: {val.shape[0]}')\n",
    "\n",
    "def make_batches(split):\n",
    "\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "Xb, Yb = make_batches('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, d_model, block_size, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.att_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1).bool())\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q = x\n",
    "        k = x\n",
    "        v = x\n",
    "        B,T,_ = x.shape \n",
    "        dk = self.d_model // self.n_heads\n",
    "\n",
    "        # linear projections\n",
    "        q = self.query(q) \n",
    "        k = self.key(k) \n",
    "        v = self.value(v) \n",
    "\n",
    "        # add number of heads\n",
    "        q = q.view(B,T,self.n_heads,dk).permute(0,2,1,3)   # B,T,h,dk\n",
    "        k = k.view(B,T,self.n_heads,dk).permute(0,2,1,3)  \n",
    "        v = v.view(B,T,self.n_heads,dk).permute(0,2,1,3)  \n",
    "        \n",
    "        # attention \n",
    "\n",
    "        x = q @ k.transpose(-2,-1) # B,h,T,dk @ B,h,dk,T --> B,h,T,T\n",
    "        x = x * dk ** -0.5 # B,h,T,T\n",
    "        x = x.masked_fill(self.mask, float('-inf')) # B,h,T,T\n",
    "        x = self.dropout(F.softmax(x, dim=(-1)))\n",
    "        x = x @ v  # B,h,T,T @ B,T,h,dv --> B,h,T,dv\n",
    "        B,h,T,dv = x.shape\n",
    "        x = x.transpose(2,1).contiguous().view(B,T,h*dv) #B,T,C\n",
    "        out = self.dropout(self.att_proj(x)) # B,T,C\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dff, dropout, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.dff = nn.Linear(d_model, dff)\n",
    "        self.out = nn.Linear(dff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.gelu(self.dff(x)) # B,T,C (dff)\n",
    "        x = self.dropout(x) \n",
    "        x = self.out(x) # B,T,C (d_model)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, d_model, dff, dropout=0.1, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.attention = MultiHeadAttention(n_heads=n_heads, \n",
    "                                            d_model=d_model, \n",
    "                                            dropout=dropout,\n",
    "                                            block_size=block_size)\n",
    "        self.ffl = FeedForward(d_model, dff, dropout)\n",
    "        self.lnorm = nn.LayerNorm(d_model)\n",
    "        self.lnorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        att_out = self.attention(x)\n",
    "        x = self.lnorm(x + self.dropout(att_out))\n",
    "        ffl_out = self.lnorm2(self.ffl(x))\n",
    "        x = x + self.dropout(ffl_out)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, d_model, dff, n_layers, dropout=0.1, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.decoder = nn.ModuleList([DecoderBlock(n_heads=n_heads,\n",
    "                                                 d_model=d_model,\n",
    "                                                 dff=dff,\n",
    "                                                 dropout=dropout,)\n",
    "                                                 for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for block in self.decoder:\n",
    "            x = block(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size, block_size, n_layers, n_heads, dff, dropout = 0.1, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(block_size, d_model)\n",
    "        self.decoder = Decoder(n_heads=n_heads,\n",
    "                                    d_model=d_model,\n",
    "                                    dff=dff,\n",
    "                                    n_layers=n_layers,\n",
    "                                    dropout=dropout)\n",
    "        self.lnorm = nn.LayerNorm(vocab_size)\n",
    "        self.out = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.embeddings(x) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        pos_emb = self.pos_embed(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.decoder(x)\n",
    "        logits = self.lnorm(self.out(x))\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(input=logits, target=targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        B, T = idx.shape\n",
    "        if T < self.block_size:\n",
    "            # pad the input with zeros if it's less than block_size\n",
    "            idx = F.pad(idx, (0, self.block_size - T))\n",
    "        for _ in range(max_new_tokens):\n",
    "            # use only the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embeddings): Embedding(65, 512)\n",
      "  (pos_embed): Embedding(256, 512)\n",
      "  (decoder): Decoder(\n",
      "    (decoder): ModuleList(\n",
      "      (0-5): 6 x DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (value): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (att_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (ffl): FeedForward(\n",
      "          (dff): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (out): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (lnorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (lnorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lnorm): LayerNorm((65,), eps=1e-05, elementwise_affine=True)\n",
      "  (out): Linear(in_features=512, out_features=65, bias=False)\n",
      ")\n",
      "Total parameters: 19099778\n"
     ]
    }
   ],
   "source": [
    "m = Model(vocab_size = vocab_size, \n",
    "          block_size = block_size, \n",
    "          n_layers = n_layers, \n",
    "          n_heads = n_heads,\n",
    "          dropout = dropout,\n",
    "          d_model = d_model,\n",
    "          dff = dff\n",
    "          ).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "n_params = sum(p.nelement() for p in m.parameters())\n",
    "print(m)\n",
    "print(f'Total parameters: {n_params}')\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(m):\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = make_batches(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 999. Training Loss: 1.875. Evaluation Loss: 2.010\n",
      "Iteration 1999. Training Loss: 1.531. Evaluation Loss: 1.724\n",
      "Iteration 2999. Training Loss: 1.399. Evaluation Loss: 1.625\n",
      "Iteration 3999. Training Loss: 1.330. Evaluation Loss: 1.568\n",
      "Iteration 4999. Training Loss: 1.280. Evaluation Loss: 1.532\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    Xb, Yb = make_batches('train')\n",
    "    logits, loss = m(Xb, Yb) # B, C\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 999:\n",
    "        l = estimate_loss(m)\n",
    "        print(f\"Iteration {epoch}. Training Loss: {l['train']:.3f}. Evaluation Loss: {l['val']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SABTYh\n",
      "\n",
      "ANoT.v\n",
      "CaNiMANO;CATsThe\n",
      "MA.HoIClon\n",
      "TovAnoAldWYounoUwCAQUbadAUjureckire:\n",
      "ThecoCllllllllloy Mut:\n",
      "Yora LAwnd:\n",
      "Pray hecos:\n",
      "Ay, madaly, y:\n",
      "I hedd, ha I y iUn have thank a!\n",
      "An GolloWes yow with teful sen his oul:\n",
      "A laidiseter.\n",
      "\n",
      " SICINIZIUS:'tis thousand the fielded not put apon.\n",
      "\n",
      "AU, my lord, your, what was you leave artichury,\n",
      "Carios Margaret\n",
      "At thousan'd friends,\n",
      "Break your heread with you.\n",
      "\n",
      "CLIFFORD:\n",
      "This sovereigns doth encre time on ans,yer well,\n",
      "and I dorom ho! Wrwithin fray the cords ways\n",
      "Supositined to his ame; 'Tush wit nowYork's these\n",
      "Will we semser'd--'Go too die the co:'\n",
      "And nell bet. See, e, good this prety.\n",
      "The eexchance mal I far a pices in the rong\n",
      "Light ort steeps.\n",
      "\n",
      "FLORIZEL:\n",
      "Hence thou rackst me be giled with him:\n",
      "What and by sigciven do crnowine best;\n",
      "But turnsling his broygives the basen\n",
      "The lifeVst that a carening of this firms,\n",
      "terrfor this armour comm break of est at his remotion inted,\n",
      "Two depostures: do yourselved to so thKenowing RosparesRumo trum.\n",
      "\n",
      "KENRCY VI:\n",
      "Sir Frenry,\n",
      "With have no chideance their natures.\n",
      "\n",
      "BRUTUS:\n",
      "'Tis not black fance as make before the viewn hannd,\n",
      "With thy have had cornatred als endulke gian'd,\n",
      "Thrun as drawful shounEd's bearZa\n",
      "And fly spry heavence feence, which of my bronocing too\n",
      "This -senath3 KINorsany say\n",
      "WARWICK:\n",
      "hat hash frourth she wides.\n",
      "\n",
      "MARIANA:\n",
      "No let the stallgot king sorrow?\n",
      "\n",
      "ROMEO:\n",
      "You die a Mortiagame.\n",
      "\n",
      "MERDERSET:\n",
      "O, I ap a him hegily; if then, no\n",
      "the as revengeth forth to hell see my wing vanks.\n",
      "\n",
      "LEONTES:\n",
      "Yourself to treason bear to procectienture,\n",
      "And you ancure--man\n",
      "And I will be dew an por a weak to bed.\n",
      "\n",
      "CATEke LLIFF:\n",
      "He ceelse with thou, my lord, the word,\n",
      "Than pited wat anoher in than half-weed suported\n",
      "She part in my better with misch. HoUmend well\n",
      "Enow that servy boded grace, as my emperale here;\n",
      "Flowe love George, and do helph\n",
      "In thy contrary but ones.\n",
      "\n",
      "HORTENSIUS:\n",
      "Thou suffiers,\n",
      "But diveres yother threen;\n",
      "Let here blow hat is true duly ot:\n",
      "not thy tale, the cusse sort you sin.\n",
      "\n",
      "STANLEY:\n",
      "Fr\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 2_000\n",
    "seed_text = \". \"\n",
    "seed_idx = torch.tensor([stoi[c] for c in seed_text], device=device).unsqueeze(0)\n",
    "predictions = m.generate(seed_idx, max_new_tokens).to(device)\n",
    "pad_len = m.block_size\n",
    "generated_text = decode(predictions[0].tolist())\n",
    "generated_text = generated_text[pad_len:]  # Remove leading padding\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
