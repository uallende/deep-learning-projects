{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 79,831,704\n",
      "Epoch: 250. Loss: 3.481. Loss: 3.533\n",
      "Epoch: 500. Loss: 2.897. Loss: 2.933\n",
      "Epoch: 750. Loss: 2.767. Loss: 2.781\n",
      "Epoch: 1000. Loss: 2.521. Loss: 2.584\n",
      "Epoch: 1250. Loss: 2.535. Loss: 2.472\n",
      "Epoch: 1500. Loss: 2.406. Loss: 2.429\n",
      "Epoch: 1750. Loss: 2.377. Loss: 2.347\n",
      "Epoch: 2000. Loss: 2.301. Loss: 2.287\n",
      "Epoch: 2250. Loss: 2.345. Loss: 2.275\n",
      "Epoch: 2500. Loss: 2.262. Loss: 2.269\n",
      "Epoch: 2750. Loss: 2.216. Loss: 2.228\n",
      "Epoch: 3000. Loss: 2.170. Loss: 2.252\n",
      "Epoch: 3250. Loss: 2.191. Loss: 2.196\n",
      "Epoch: 3500. Loss: 2.149. Loss: 2.161\n",
      "Epoch: 3750. Loss: 2.116. Loss: 2.192\n",
      "Epoch: 4000. Loss: 2.117. Loss: 2.149\n",
      "Epoch: 4250. Loss: 2.104. Loss: 2.146\n",
      "Epoch: 4500. Loss: 2.204. Loss: 2.163\n",
      "Epoch: 4750. Loss: 2.144. Loss: 2.112\n",
      "Epoch: 5000. Loss: 2.101. Loss: 2.108\n",
      "Epoch: 5250. Loss: 2.127. Loss: 2.112\n",
      "Epoch: 5500. Loss: 2.152. Loss: 2.093\n",
      "Epoch: 5750. Loss: 2.114. Loss: 2.127\n",
      "Epoch: 6000. Loss: 2.168. Loss: 2.084\n",
      "Epoch: 6250. Loss: 2.117. Loss: 2.105\n",
      "Epoch: 6500. Loss: 2.128. Loss: 2.139\n",
      "Epoch: 6750. Loss: 2.078. Loss: 2.141\n",
      "Epoch: 7000. Loss: 2.170. Loss: 2.094\n",
      "Epoch: 7250. Loss: 2.109. Loss: 2.117\n",
      "Epoch: 7500. Loss: 2.125. Loss: 2.162\n",
      "Epoch: 7750. Loss: 2.077. Loss: 2.123\n",
      "Epoch: 8000. Loss: 2.135. Loss: 2.156\n",
      "Epoch: 8250. Loss: 2.049. Loss: 2.219\n",
      "Epoch: 8500. Loss: 2.177. Loss: 2.146\n",
      "Epoch: 8750. Loss: 2.129. Loss: 2.181\n",
      "Epoch: 9000. Loss: 2.262. Loss: 2.231\n",
      "Epoch: 9250. Loss: 2.168. Loss: 2.117\n",
      "Epoch: 9500. Loss: 2.230. Loss: 2.191\n",
      "Epoch: 9750. Loss: 2.183. Loss: 2.177\n",
      "Epoch: 10000. Loss: 2.178. Loss: 2.239\n",
      "Epoch: 10250. Loss: 2.238. Loss: 2.239\n",
      "Epoch: 10500. Loss: 2.201. Loss: 2.296\n",
      "Epoch: 10750. Loss: 2.227. Loss: 2.275\n",
      "Epoch: 11000. Loss: 2.235. Loss: 2.216\n",
      "Epoch: 11250. Loss: 2.243. Loss: 2.272\n",
      "Epoch: 11500. Loss: 2.214. Loss: 2.248\n",
      "Epoch: 11750. Loss: 2.244. Loss: 2.332\n",
      "Epoch: 12000. Loss: 2.297. Loss: 2.277\n",
      "Epoch: 12250. Loss: 2.329. Loss: 2.301\n",
      "Epoch: 12500. Loss: 2.298. Loss: 2.308\n",
      "Epoch: 12750. Loss: 2.278. Loss: 2.241\n",
      "Epoch: 13000. Loss: 2.347. Loss: 2.355\n",
      "Epoch: 13250. Loss: 2.334. Loss: 2.367\n",
      "Epoch: 13500. Loss: 2.431. Loss: 2.438\n",
      "Epoch: 13750. Loss: 2.359. Loss: 2.395\n",
      "Epoch: 14000. Loss: 2.357. Loss: 2.437\n",
      "Epoch: 14250. Loss: 2.333. Loss: 2.348\n",
      "Epoch: 14500. Loss: 2.397. Loss: 2.373\n",
      "Epoch: 14750. Loss: 2.412. Loss: 2.390\n",
      "Epoch: 15000. Loss: 2.404. Loss: 2.434\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')  \n",
    "\n",
    "from Classes.tokenizer import Tokenizer as T\n",
    "from utils_hp_search import Trainer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Possible hyperparameters to search over\n",
    "# block_size_values = [172]\n",
    "# dropout_values = [0.2]\n",
    "# lr_values = [3e-4]\n",
    "run_count = 0\n",
    "batch_size_values = [40]\n",
    "block_size = 128\n",
    "batch_size = 48\n",
    "n_heads = 2\n",
    "n_layers = 8\n",
    "d_model = 768\n",
    "dropout = 0.2\n",
    "learning_rate = 3e-4\n",
    "epochs = 15_000\n",
    "eval_iters = 10\n",
    "vocab_size = 15_000\n",
    "\n",
    "# Define static attributes\n",
    "static_attributes = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'n_heads': n_heads,\n",
    "    'n_layers': n_layers,\n",
    "    'device': device,\n",
    "    'd_model': d_model,\n",
    "    #'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'eval_iters': eval_iters,\n",
    "    'learning_rate': learning_rate,\n",
    "    'dropout': dropout,\n",
    "    'block_size': block_size,\n",
    "    'dff': d_model * 4,\n",
    "}\n",
    "\n",
    "train = torch.load('data/tokenized_inputs/tns_chunk_0.pt')\n",
    "val = torch.load('data/tokenized_inputs/val.pt')\n",
    "\n",
    "# for block_size in block_size_values:\n",
    "#     for dropout in dropout_values:\n",
    "#         for learning_rate in lr_values:\n",
    "\n",
    "for batch_size in batch_size_values:\n",
    "            dff = d_model * 4  # Assuming dff is always 4 * d_model\n",
    "            \n",
    "            variable_attributes = {\n",
    "                'batch_size': batch_size,\n",
    "            }\n",
    "            \n",
    "            all_attributes = {**static_attributes, **variable_attributes}\n",
    "            trainer = Trainer(**all_attributes)\n",
    "            trainer.load_data(train=train, val=val)\n",
    "            trainer.train_model()\n",
    "            run_count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
